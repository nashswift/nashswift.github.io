<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="What matters is not your ability, but your choice.">
<meta property="og:type" content="website">
<meta property="og:title" content="长野原烟花店">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="长野原烟花店">
<meta property="og:description" content="What matters is not your ability, but your choice.">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="nashswift">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>长野原烟花店</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">长野原烟花店</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/06/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nashswift">
      <meta itemprop="description" content="What matters is not your ability, but your choice.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长野原烟花店">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/06/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">周志华《机器学习》（AKA：西瓜书）笔记整理-第三章-线性模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-06 09:42:26" itemprop="dateCreated datePublished" datetime="2023-06-06T09:42:26+08:00">2023-06-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-07 11:20:31" itemprop="dateModified" datetime="2023-06-07T11:20:31+08:00">2023-06-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-study/" itemprop="url" rel="index"><span itemprop="name">ML study</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>机器学习三要素：</p>
<p>模型——根据具体问题，确定假设空间</p>
<p>策略——根据评价标准，确定选取最优模型的策略</p>
<p>算法——求解损失函数，确定最终模型</p>
</blockquote>
<h1 id="3-1-基本形式"><a href="#3-1-基本形式" class="headerlink" title="3.1 基本形式"></a>3.1 基本形式</h1><p><strong>线性模型（linear model）</strong>：试图学得一个通过属性的线性组合来进行预测的函数。</p>
<ul>
<li>一般形式</li>
</ul>
<script type="math/tex; mode=display">
f(x)=w_1x_1+w_2x_2+...+w_dx_d+b</script><ul>
<li>向量形式</li>
</ul>
<script type="math/tex; mode=display">
f(x)=w^Tx+b</script><h1 id="3-2-线性回归-linear-regression"><a href="#3-2-线性回归-linear-regression" class="headerlink" title="3.2 线性回归(linear regression)"></a>3.2 线性回归(linear regression)</h1><h2 id="3-2-1-一元线性回归（只有一个输入属性）"><a href="#3-2-1-一元线性回归（只有一个输入属性）" class="headerlink" title="3.2.1 一元线性回归（只有一个输入属性）"></a>3.2.1 一元线性回归（只有一个输入属性）</h2><p><strong>一元线性回归</strong>试图学得：</p>
<script type="math/tex; mode=display">
f(x_i)=wx_i+b, 使得f(x_i\simeq y_i)</script><p>采用均方误差作为性能度量，并对均方误差进行最小化：</p>
<script type="math/tex; mode=display">
(w^*,b^*)= \underset{(w, b)}{\arg\min}{\sum_{i=1}^m{(f(x_i)-y_i)^2}}=\underset{(w,b)}{\arg\min}{\sum_{i=1}^m{(y_i-wx_i-b)^2}}</script><p>采用最小二乘法分别对w和b求导并赋0，可以得到w和b最优解的闭式解：</p>
<script type="math/tex; mode=display">
w=\frac{\sum_{i=1}^m{y_i(x_i-\bar x)}}{\sum_{i=1}^mx_i^2-\frac{1}{m}(\sum_{i=1}^m{x_i})^2}</script><script type="math/tex; mode=display">
b=\frac{1}{m}\sum_{i=1}^{m}{(y_i-wx_i)}</script><h2 id="3-2-2-多元线性回归（有d个输入属性）"><a href="#3-2-2-多元线性回归（有d个输入属性）" class="headerlink" title="3.2.2 多元线性回归（有d个输入属性）"></a>3.2.2 多元线性回归（有d个输入属性）</h2><p><strong>多元线性回归</strong>试图学得：</p>
<script type="math/tex; mode=display">
f(x_i)=w^Tx_i+b_i,使得f(x_i)\simeq y_i</script><p>将w和b吸收入向量形式<script type="math/tex">\hat w=(w;b)</script>，则上式可化为：</p>
<script type="math/tex; mode=display">
f(x)=\hat w^TX,其中\hat w=[b,w_1,w_2,...,w_d]^T,X=[1,x_1,x_2,...,x_d]^T</script><ul>
<li><strong>方法一：梯度下降法（Gradient Descent）</strong></li>
</ul>
<p>代价函数（cost function）：<script type="math/tex">J(\hat w)=\frac{1}{2m}\sum_{i=1}^{m}{(f(x_i)-y_i)^2}</script></p>
<p>梯度下降：</p>
<p>Repeat {</p>
<p>​    <script type="math/tex">\hat w_j:=\hat w_j-\alpha\frac{\partial}{\partial\hat{w}_j}J(\hat{w})</script>    for every j = 0, 1, …, d</p>
<p>} until convergence</p>
<p>其中<script type="math/tex">\alpha</script>为学习率。若<script type="math/tex">\alpha</script>过小，收敛速度太慢；若<script type="math/tex">\alpha</script>过大，则容易跳过极值点。选取<script type="math/tex">\alpha</script>时可以隔10倍取值进行尝试。</p>
<p>缺点：需要多次选择学习率<script type="math/tex">\alpha</script>，运行多次确定最好的值；需要更多次的迭代。</p>
<ul>
<li><strong>方法二：正规方程法（Normal Equation）</strong></li>
</ul>
<p>优化目标：<script type="math/tex">\hat{w}^*=\underset{\hat{w}}{\arg\min}(y-X\hat{w})^T(y-X\hat{w})</script></p>
<p>对<script type="math/tex">\hat{w}</script>求导并赋0得：<script type="math/tex">\hat{w}^*=(X^TX)^{-1}X^Ty</script></p>
<p>缺点：<script type="math/tex">(X^TX)^{-1}</script>求解麻烦，时间复杂度高，特征量越多计算越复杂。</p>
<blockquote>
<p>梯度下降法和正规方程法对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">梯度下降法</th>
<th style="text-align:center">正规方程法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">需要选择学习率</td>
<td style="text-align:center">不需要选择学习率</td>
</tr>
<tr>
<td style="text-align:center">需要迭代求解</td>
<td style="text-align:center">一蹴而就</td>
</tr>
<tr>
<td style="text-align:center">特征数量比较大时可以使用</td>
<td style="text-align:center">需要计算方程，时间复杂度高</td>
</tr>
</tbody>
</table>
</div>
<p><strong>若特征量d很大（大于1000），则选择梯度下降法，否则选择正规方程法。</strong></p>
</blockquote>
<h1 id="3-3-对数几率回归（逻辑回归）"><a href="#3-3-对数几率回归（逻辑回归）" class="headerlink" title="3.3 对数几率回归（逻辑回归）"></a>3.3 对数几率回归（逻辑回归）</h1><p><strong>对数几率回归（logistic regression）</strong>：虽然名字叫做回归，但是实际上解决的是分类问题。主要思想是找到一个单调可微函数将分类任务的真实标记y和线性回归模型的预测值f(x)联系起来。</p>
<p><strong>联系函数（link function）</strong>：</p>
<ul>
<li><strong>单位阶跃函数（unit-step function）</strong>:</li>
</ul>
<script type="math/tex; mode=display">
y=
\begin{cases}
0,\quad z<0\\
0.5,\quad z=0\\
1,\quad z>0
\end{cases}</script><p>注：该函数是分段函数，不便于求解，因此不能直接作为联系函数来使用。</p>
<ul>
<li><strong>对数几率函数（logistic function）</strong>：</li>
</ul>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-z}}</script><p>注：该函数是一种<strong>“Sigmoid”函数</strong>，将z值转换为一个接近0或1的y值，并且在z=0附近变化很陡，故采用。将<script type="math/tex">z=w^Tx+b</script>代入可得<script type="math/tex">y=\frac{1}{1+e^{-(w^Tx+b)}}</script>。</p>
<p><img src="/2023/06/06/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/单位阶跃函数和对数几率函数.png" alt="单位阶跃函数和对数几率函数"></p>
<p>可以通过<strong>“极大似然法”（maximum likelihood method, MLE）</strong>来估计参数w和b（合起来写作<script type="math/tex">\hat{w}</script>），并采用<strong>对数似然法（log-likelihood）</strong>简化计算，得到代价函数：</p>
<script type="math/tex; mode=display">
J(\hat{w})=\frac{1}{m}\sum_{i=1}^{m}{cost(f(x_i),y_i)}</script><script type="math/tex; mode=display">
其中：cost(f(x), y)=
\begin{cases}
-log(f(x)), \quad y=1\\
-log(1-f(x)), \quad y=0
\end{cases}</script><p>将式（12）的分段函数写成一个式子（方法不唯一）：</p>
<script type="math/tex; mode=display">
cost(f(x),y)=-ylog(f(x))-(1-y)log(1-f(x))</script><p>最终<strong>优化目标</strong>是：</p>
<script type="math/tex; mode=display">
\hat{w}=\underset{\hat{w}}{\arg\min}\frac{1}{m}\sum_{i=1}^{m}\Big[{y_ilog(f(x_i))+(1-y_i)log(1-f(x_i))}\Big]</script><ul>
<li><strong>方法一：梯度下降法（和线性回归的梯度下降一样，只不过代价函数不同）</strong></li>
</ul>
<p>Repeat {</p>
<p>​    <script type="math/tex">\hat w_j:=\hat w_j-\alpha\frac{\partial}{\partial\hat{w}_j}J(\hat{w})</script>    for every j = 0, 1, …, d</p>
<p>} until convergence</p>
<ul>
<li><strong>方法二：牛顿法（Newton Method）</strong></li>
</ul>
<p>第t+1轮迭代解的更新公式是：</p>
<script type="math/tex; mode=display">
\hat{w}^{t+1}:=\hat{w}^t-\Big(\frac{\partial^2J(\hat{w})}{\partial{\hat{w}}\partial{\hat{w}^T}}\Big)^{-1}\frac{\partial{J(\hat{w})}}{\partial\hat{w}}</script><blockquote>
<p>其他高级优化算法</p>
<ul>
<li>共轭梯度法</li>
<li>BFGS</li>
<li>L-BFGS（在有限内存中进行BFGS算法）</li>
</ul>
<p>优点：不需要手动选择学习率<script type="math/tex">\alpha</script>，收敛速度远快于梯度下降</p>
<p>缺点：更为复杂</p>
</blockquote>
<h1 id="（补充）广义线性模型（generalized-linear-model-GLM）"><a href="#（补充）广义线性模型（generalized-linear-model-GLM）" class="headerlink" title="（补充）广义线性模型（generalized linear model, GLM）"></a>（补充）广义线性模型（generalized linear model, GLM）</h1><p><strong>1. 指数族分布（exponential families of distributions）</strong>：具有如下特定形式的概率分布的参数集合</p>
<script type="math/tex; mode=display">
P_X(x|\theta)=h(x)exp(\eta(\theta)T(x)-A(\theta))</script><p>其中：x是数据（data），<script type="math/tex">\theta</script>是自然参数（natural parameter），T(x)是充分统计量（sufficiant statistic），h(x)是基本度量（Base measure），<script type="math/tex">A(\theta)</script>是log配分函数（log partition-function）</p>
<blockquote>
<ol>
<li><p>指数族分布满足最大熵原理</p>
</li>
<li><p>诸如高斯分布、伯努利分布、二项分布、泊松分布、Beta分布、Gamma分布等一系列分布都属于指数族分布</p>
</li>
</ol>
</blockquote>
<p><strong>2. 广义线性模型（GLM）的运用</strong>：</p>
<p>GLM的<strong>三个假设</strong>：</p>
<ol>
<li><p><script type="math/tex">y|x;\hat{w}</script>满足一个以<script type="math/tex">\eta</script>为自然参数的指数族分布，那么可以求得<script type="math/tex">\eta</script>的表达式；</p>
</li>
<li><p>给定x，我们的目标是预测T(y)的期望值，大多数情况下可以认为T(y)=y，因此我们实际上是要确定一个h(x)使得h(x)=E[y|x]；</p>
</li>
<li><p>重新表示<script type="math/tex">\eta=\hat{w}^Tx</script>。</p>
</li>
</ol>
<p><strong>具体步骤</strong>：</p>
<p>Step1：核心假设<script type="math/tex">y\sim exponential family</script></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">样本标签y的类型</th>
<th style="text-align:center">选择的指数族分布</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Real</td>
<td style="text-align:center">Gaussian</td>
</tr>
<tr>
<td style="text-align:center">Binary</td>
<td style="text-align:center">Bernoulli</td>
</tr>
<tr>
<td style="text-align:center">Count</td>
<td style="text-align:center">Poisson</td>
</tr>
<tr>
<td style="text-align:center"><script type="math/tex">R^2</script></td>
<td style="text-align:center">Gamma, Exponential</td>
</tr>
<tr>
<td style="text-align:center">Distn</td>
<td style="text-align:center">Beta, Dirichlet</td>
</tr>
</tbody>
</table>
</div>
<p>Step2：将具体分布改写为指数族分布的形式，利用响应函数（response function）和连接函数（link function）找到规范参数（canonical parameter）和自然参数<script type="math/tex">\eta</script>之间的关系</p>
<p>Step3：将自然参数<script type="math/tex">\eta</script>用<script type="math/tex">\hat{w}^Tx</script>代替</p>
<p>Step4：模型求解</p>
<blockquote>
<p>注：自然参数<script type="math/tex">\eta</script>以不同的映射函数与其他概率分布函数中的参数（规范参数）发生联系，从而得到不同的模型，广义线性模型正是将指数族分布中的所有成员（每个成员正好有一个这样的联系）都作为线性模型的拓展，通过各种非线性的连接函数将线性函数映射到其他空间，从而大大扩大了线性模型可解决的问题。</p>
</blockquote>
<p><strong>3. 广义线性模型运用举例——softmax回归</strong>:</p>
<p>softmax回归也是广义线性模型的一种实现，主要是用于解决多分类任务。假设y的估值概率属于指数分布族中的多项式分布，即<script type="math/tex">y|x,\hat{w}\sim Mult(\phi)</script>，通过一系列推导（<em>参见：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e2cf48dd8e40">广义线性模型和softmax回归</a></em>）可以得到最终回归模型为：</p>
<script type="math/tex; mode=display">
f(x;\hat{w})=\left[
\begin{matrix}
\frac{e^{\hat{w}_1^Tx}}{\sum_{i=1}^{d}{e^{\hat{w_i}^Tx}}}\\
\frac{e^{\hat{w}_2^Tx}}{\sum_{i=1}^{d}{e^{\hat{w_i}^Tx}}}\\
\vdots\\
\frac{e^{\hat{w}_{d-1}^Tx}}{\sum_{i=1}^{d}{e^{\hat{w_i}^Tx}}}
\end{matrix}
\right]</script><p>利用极大似然估计法，得到回归模型的交叉熵代价函数：</p>
<script type="math/tex; mode=display">
J(\hat{w})=-\frac{1}{m}\Bigg[\sum_{i=1}^{m}\sum_{j=1}^d\amalg(y_i=j)log\frac{e^{\hat{w}_j^Tx_i}}{\sum_{l=1}^{k}{e^{\hat{w}_l^Tx_i}}}\Bigg]</script><p>最后采用梯度下降法对模型进行求解，更新公式为：</p>
<script type="math/tex; mode=display">
\hat w_j:=\hat w_j-\alpha\frac{\partial}{\partial\hat{w}_j}J(\hat{w})</script><h1 id="3-4-线性判别分析（Linear-Discriminate-Analysis-LDA）"><a href="#3-4-线性判别分析（Linear-Discriminate-Analysis-LDA）" class="headerlink" title="3.4 线性判别分析（Linear Discriminate Analysis, LDA）"></a>3.4 线性判别分析（Linear Discriminate Analysis, LDA）</h1><p><strong>主要思想</strong>：给定训练样例集，设法将样本点投影到一条直线上，使得同类样本的投影点尽可能接近、异类样本的投影点尽可能远离；在对新样本分类时也是先将其投影到这条直线上，再根据投影点的位置作出判断。</p>
<p><img src="/2023/06/06/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/LDA.png" alt="LDA"></p>
<h2 id="3-4-1-二分类任务"><a href="#3-4-1-二分类任务" class="headerlink" title="3.4.1 二分类任务"></a>3.4.1 二分类任务</h2><p>令<script type="math/tex">X_i</script>、<script type="math/tex">\mu_i</script>、<script type="math/tex">\Sigma_i</script>表示第i类样本的集合、均值向量、协方差矩阵，并将样本点投影至直线<script type="math/tex">y=w^Tx</script>上。</p>
<ul>
<li>类内散度矩阵（within-class scatter matrix）：刻画同类样本投影点之间的距离</li>
</ul>
<script type="math/tex; mode=display">
S_w=\Sigma_0+\Sigma_1=\sum_{x\in X_0}{(x-\mu_0)^T(x-\mu_0)}+\sum_{x\in X_1}{(x-\mu_1)^T(x-\mu_1)}</script><ul>
<li>类间散度矩阵（between-class scatter matrix）：刻画异类样本投影点中心之间的距离</li>
</ul>
<script type="math/tex; mode=display">
S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T</script><p><strong>最优化目标</strong>为：</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{w}{min}\ -w^TS_bw\\
s.t.\ w^TS_ww=1
\end{split}</script><p>采用拉格朗日乘子法求解，得到：</p>
<script type="math/tex; mode=display">
w^*=S_w^{-1}(\mu_0-\mu_1)</script><blockquote>
<p>上式可对<script type="math/tex">S_w</script>进行奇异值分解的方式求解<script type="math/tex">S_w^{-1}</script>。</p>
</blockquote>
<h2 id="3-4-2-多分类任务"><a href="#3-4-2-多分类任务" class="headerlink" title="3.4.2 多分类任务"></a>3.4.2 多分类任务</h2><ul>
<li>全局散度矩阵：</li>
</ul>
<script type="math/tex; mode=display">
S_t=S_b+S_w=\sum_{i=1}^m{(x_i-\mu)(x_i-\mu)^T}</script><p>显然，多分类LDA可以有多种实现方法：使用<script type="math/tex">S_b</script>、<script type="math/tex">S_w</script>、<script type="math/tex">S_t</script>三者中的任意两个即可，通常采用如下<strong>最优化目标</strong>：</p>
<script type="math/tex; mode=display">
\underset{W}{max}\frac{tr(W^TS_bW)}{tr(W^TS_wW)}</script><p>W的闭式解则是<script type="math/tex">S_w^{-1}S_b</script>的d‘（<script type="math/tex">d’\leq N-1</script>）个最大非零广义特征值所对应的特征向量组成的矩阵。</p>
<blockquote>
<p>可以认为这d‘个特征向量是“最具特征的”。</p>
</blockquote>
<h1 id="3-5-多分类学习"><a href="#3-5-多分类学习" class="headerlink" title="3.5 多分类学习"></a>3.5 多分类学习</h1><p><strong>基本策略</strong>：利用二分类学习器解决多分类问题</p>
<ul>
<li>“一对一”（OvO）：将N个类别两两配对，从而产生N(N-1)/2个二分类任务，最终结果通过投票产生。</li>
<li>“一对多”（OvR）：每次将一个类的样例作为正例，其余类的样例作为反例来训练N个分类器。</li>
</ul>
<p><img src="/2023/06/06/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/OvO和OvR.png" alt="OvO和OvR"></p>
<ul>
<li>“多对多”（MvM）：每次将若干个类作为正类，若干个其他类作为反类。</li>
</ul>
<h1 id="3-6-类别不平衡问题"><a href="#3-6-类别不平衡问题" class="headerlink" title="3.6 类别不平衡问题"></a>3.6 类别不平衡问题</h1><p><strong>类别不平衡（class-imbalance）</strong>：是指分类任务中不同类别的训练样例数目差别很大。</p>
<p><strong>基本策略</strong>：再缩放（rescaling）</p>
<p><strong>解决方法</strong>：</p>
<ol>
<li>对较多的样本进行欠采样，eg. SMOTE（插值产生额外样例）</li>
<li>对较少的样本进行过采样，eg. EasyEnsemble（利用集成学习将样例划分为若干个集合）</li>
<li>阈值移动（threshold-moving）</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nashswift">
      <meta itemprop="description" content="What matters is not your ability, but your choice.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长野原烟花店">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/" class="post-title-link" itemprop="url">《机器学习》第二章 模型评估与选择</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-05 13:09:22" itemprop="dateCreated datePublished" datetime="2023-06-05T13:09:22+08:00">2023-06-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-06 00:28:59" itemprop="dateModified" datetime="2023-06-06T00:28:59+08:00">2023-06-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-study/" itemprop="url" rel="index"><span itemprop="name">ML study</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="2-1-经验误差与过拟合"><a href="#2-1-经验误差与过拟合" class="headerlink" title="2.1 经验误差与过拟合"></a>2.1 经验误差与过拟合</h1><p><strong>错误率（error rate）</strong>：分类错误的样本数占样本总数的比例。如果在m个样本中有a个样本分类错误，则错误率为$E = \frac{a}{m}$ 。</p>
<p><strong>精度（accuracy）</strong>：精度=1-错误率。<br><strong>误差（error）</strong>：学习器的实际预测输出与样本的真实输出之间的差异。<br><strong>训练误差（training error）/经验误差（empirical error）</strong>：学习器在训练集上的误差。<br><strong>泛化误差（generalization error）</strong>：学习器在新样本上的误差（我们希望得到泛化误差小的学习器）。</p>
<p><strong>过拟合</strong>：在训练集上表现很好，但在测试集上表现不好（学习能力太强，不可避免，只能缓解）。<br><strong>欠拟合</strong>：在训练集和测试集上表现都不好（学习能力太弱，加大学习）。</p>
<blockquote>
<p>解决过拟合的方法：</p>
<p>​    1.降低参数空间的维度（如剪枝、权重共享等）<br>​    2.降低每个参数维度上的有效规模（如正则化、早停法、训练更多数据等）</p>
<p>解决欠拟合的方法：</p>
<p>​    1.增加新的特征或多项式特征<br>​    2.使用非线性模型等更复杂的模型<br>​    3.使用集成学习方法，如Bagging等</p>
</blockquote>
<p><strong>补充</strong>：</p>
<p>P类问题（Polynominal problem）：存在多项式时间算法问题。</p>
<p>NP类问题（Nondeterministic Polynominal-hard problem）：能在多项式时间内验证得出一个正确的解。</p>
<p>NP难问题（NP-hard problem）：无法得到多项式级的算法。</p>
<h1 id="2-2-评估方法"><a href="#2-2-评估方法" class="headerlink" title="2.2 评估方法"></a>2.2 评估方法</h1><p><strong>测试集（testing set）</strong>：来测试学习器对新样本的判别能力。</p>
<ul>
<li>从样本真实分布中独立同分布（iid）采样而得</li>
<li>尽可能与训练集互斥</li>
</ul>
<p><strong>测试误差（testing error）</strong>：作为泛化误差的近似。</p>
<h2 id="2-2-1-留出法（hold-out）"><a href="#2-2-1-留出法（hold-out）" class="headerlink" title="2.2.1 留出法（hold-out）"></a>2.2.1 留出法（hold-out）</h2><p>直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S（$70\%-80\% $），另一个作为测试集T（$20\%-30\%$）。</p>
<blockquote>
<p>以二分类任务为例子，假定D中有1000个样本，划分后S包含700个样本T包含300个样本，用S进行训练之后，若模型在T上有90个样本分类错误，那么错误率为$(90/300)\times100\% = 30\%$，相应的精度为$1-30\%= 70\%$。</p>
</blockquote>
<p><strong>注意</strong>：单次使用留出法时得到的估计结果往往不够稳定可靠，因此一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。</p>
<h2 id="2-2-2-交叉验证法（cross-validation）"><a href="#2-2-2-交叉验证法（cross-validation）" class="headerlink" title="2.2.2 交叉验证法（cross validation）"></a>2.2.2 交叉验证法（cross validation）</h2><p>将数据集D划分为k个大小相似的互斥子集，每个子集都从D中通过分层残阳得到（尽可能保持数据分布一致性），然后每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，最终返回的是k个测试结果的均值。</p>
<ul>
<li><p><strong>k折交叉验证（k-fold cross validation）</strong>：k最常用的取值是10，此外还常用的有5、20等。</p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/10折交叉验证示意图.png" alt="10折交叉验证示意图"></p>
<center>10折交叉验证示意图</center>

<p>为减少因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终评估结果是折p次k折交叉验证结果的均值。</p>
</li>
<li><p><strong>留一法（Leave-One-Out，简称LOO）</strong>：k=m，m为样本个数，数据集较大时不推荐使用。</p>
</li>
</ul>
<h2 id="2-2-3-自助法（bootstrapping）"><a href="#2-2-3-自助法（bootstrapping）" class="headerlink" title="2.2.3 自助法（bootstrapping）"></a>2.2.3 自助法（bootstrapping）</h2><p>直接以自助手采样（bootstrap sampling）为基础。在包含m个样本的数据集D中<strong>有放回</strong>地采样m次得到数据集D’，初始数据集D中约有36.8%的样本未出现在采样数据集D’中，于是我们把D’作为训练集，D\D‘作为测试集。这样，我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试，这样的测试结果也叫做<strong>“包外估计”（out-of-bag estimate）</strong>。</p>
<p><strong>缺点</strong>：自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。</p>
<h2 id="2-2-4-调参与最终模型"><a href="#2-2-4-调参与最终模型" class="headerlink" title="2.2.4 调参与最终模型"></a>2.2.4 调参与最终模型</h2><p><strong>调参（paraemeter tuning）</strong>：对算法的参数进行设定。</p>
<p><strong>验证集（validation set）</strong>：将训练数据另外划分为训练集和验证集，基于验证集上的性能进行模型参数选择和调参。</p>
<h1 id="2-3-性能度量"><a href="#2-3-性能度量" class="headerlink" title="2.3 性能度量"></a>2.3 性能度量</h1><p><strong>性能度量（performance measure）</strong>：衡量模型泛化能力的评价标准（相对的）。</p>
<p>常用的性能度量——<strong>均方误差（mean squared error）</strong>:</p>
<ul>
<li><p>离散：</p>
<script type="math/tex; mode=display">
E(f;D) = \frac{1}{m}\sum_{i=1}^{m}{(f(x_{i})-y_{i})^2}</script></li>
<li><p>连续：</p>
<script type="math/tex; mode=display">
E(f;D)=\int_{x\sim D}{(f(x)-y)^2}p(x)dx</script></li>
</ul>
<h2 id="2-3-1-错误率与精度"><a href="#2-3-1-错误率与精度" class="headerlink" title="2.3.1 错误率与精度"></a>2.3.1 错误率与精度</h2><p><strong>错误率</strong>：分类错误的样本数占样本总数的比例。</p>
<ul>
<li><p>离散：</p>
<script type="math/tex; mode=display">
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}{\amalg(f(x_i)\neq y_i)}</script></li>
<li><p>连续：</p>
<script type="math/tex; mode=display">
E(f;D)=\int_{x\sim D}{\amalg(f(x)\neq y)p(x)dx}</script></li>
</ul>
<p><strong>精度</strong>：分类正确的样本数占样本总数的比例。</p>
<ul>
<li><p>离散：</p>
<script type="math/tex; mode=display">
acc(f;D)=\frac{1}{m}\sum_{i=1}^{m}{\amalg(f(x_i)=y_i)}=1-E(f;D)</script></li>
<li><p>连续：</p>
<script type="math/tex; mode=display">
acc(f;D)=\int_{x\sim D}{\amalg(f(x)=y)p(x)dx}=1-E(f;D)</script></li>
</ul>
<h2 id="2-3-2-查准率、查全率与F1"><a href="#2-3-2-查准率、查全率与F1" class="headerlink" title="2.3.2 查准率、查全率与F1"></a>2.3.2 查准率、查全率与F1</h2><ul>
<li><p><strong>真正例（true positive）</strong>：TP</p>
</li>
<li><p><strong>假正例（false positive）</strong>：FP</p>
</li>
<li><p><strong>真反例（true negative）</strong>：TN</p>
</li>
<li><p><strong>假反例（false negative）</strong>：FN</p>
</li>
</ul>
<p><strong><em>注：TP + FP + TN + FN = 样例总数</em></strong></p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/分类混淆矩阵.png" alt="分类混淆矩阵"></p>
<p><strong>查准率（precision）</strong>：在所有预测的正例中真正例的比例（“所查有多准”）。</p>
<script type="math/tex; mode=display">
P = \frac{TP}{TP+FP}</script><p>注：分母TP+FP为预测为真的总数</p>
<p><strong>查全率（recall）</strong>：所有的正例中有多少被查出来了（“所查有多全”）。</p>
<script type="math/tex; mode=display">
R = \frac{TP}{TP+FN}</script><p>注：分母TP+FN为实际为真的总数</p>
<p><strong>P-R曲线</strong>：查准率和查全率是两个互斥的变量，查全率高时查准率往往偏低，查准率高时查全率往往偏低。</p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/P-R曲线和平衡点示意图.png" alt="P-R曲线和平衡点示意图"></p>
<p><strong>度量学习器优劣的方法：</strong></p>
<ol>
<li><p>直接法：若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可以断言后者的性能优于前者</p>
</li>
<li><p>面积法：比较两个学习器P-R曲线下面积的大小，面积大的曲线性能更好</p>
</li>
<li><p>平衡点法：平衡点（BEP）是“查准率=查全率”时的取值，平衡点更大者性能更好</p>
</li>
<li><p>F1度量：</p>
<script type="math/tex; mode=display">
F1 = \frac{2\times P\times R}{P+R}</script></li>
<li><p>F1度量的更一般形式 ——<script type="math/tex">F_\beta</script>度量：</p>
<script type="math/tex; mode=display">
F_\beta=\frac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}</script><p>注：<script type="math/tex">\beta=1</script>时退化为F1度量，<script type="math/tex">\beta>1</script>时查全率影响更大，<script type="math/tex">\beta<1</script>时查准率影响更大</p>
</li>
<li><p>宏查准率（macro-P）、宏查全率（macro-R）、宏F1（macro-F1）：先计算P、R、F1，再取平均</p>
</li>
<li><p>微查准率（micro-P）、微查全率（micro-R）、微F1（micro-F1）：先取平均，再计算P、R、F1</p>
<p>注：方法6和方法7性能差别不大</p>
</li>
</ol>
<h2 id="2-3-3-ROC和AUC"><a href="#2-3-3-ROC和AUC" class="headerlink" title="2.3.3 ROC和AUC"></a>2.3.3 ROC和AUC</h2><p>很多学习器是诶测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值（threshold）比较，若大于阈值则为正类，否则为反类。若更重视查准率，则可以选择排序中靠前的位置截断；若更重视查全率，则可以选择靠后的位置截断。因此，<strong>排序本身的质量好坏，体现了综合考虑学习器在不同任务下期望泛化性能的好坏</strong>。</p>
<p><strong>ROC（受试者工作特征Receiver Operating Characteristic）曲线</strong>：</p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/ROC曲线和AUC.png" alt="ROC曲线和AUC"></p>
<blockquote>
<p><strong>ROC曲线理解：</strong></p>
<p>真正例率TPR：抓住坏人的比例</p>
<p>假正例率FPR：误伤好人的比例</p>
<p>1处：好人全放走，坏人全抓到（理想情况）</p>
<p>2处：不论好人坏人全部抓</p>
<p>3处：好人全抓了，坏人全放走（最差情况）</p>
<p>4处：不论好人坏人全部放</p>
<p><strong>因此ROC曲线越接近于点（0, 1）越好，越接近对角线（随机猜测）越差。</strong></p>
</blockquote>
<p><strong>AUC（Area Under ROC Curve）</strong>：ROC曲线下的面积（学习器比较的合理判据）。</p>
<ul>
<li><p>几何法：</p>
<script type="math/tex; mode=display">
AUC=\frac{1}{2}\sum_{i=1}^{m-1}{(x_{i+1}-x_i)(y_{i+1}+y_i)}</script></li>
<li><p>损失法：</p>
<script type="math/tex; mode=display">
AUC = 1-l_{rank}</script><script type="math/tex; mode=display">
l_{rank}=\frac{1}{m^+\times m^-}\sum_{x^+\in D^+}\sum_{x^-\in D^-}\Big(\amalg(f(x^+)<f(x^-))+\frac{1}{2}\amalg(f(x^+)=f(x^-))\Big)</script><p>注：<script type="math/tex">l_{rank}</script>是排序损失，对应ROC曲线之上的面积。考虑每一对正例和反例，若正例的预测值小于反例，则记1个“罚分”；若相等，则记0.5个“罚分”。</p>
</li>
</ul>
<h2 id="2-3-4-代价敏感错误率与代价曲线"><a href="#2-3-4-代价敏感错误率与代价曲线" class="headerlink" title="2.3.4 代价敏感错误率与代价曲线"></a>2.3.4 代价敏感错误率与代价曲线</h2><p><strong>非均等代价（unequal cost）</strong>：把第i类的样本预测为第j类的代价和把第j类的样本预测为第i类的代价不相等（<script type="math/tex">cost_{ij}\neq cost_{ji}</script>）。</p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/二分类代价矩阵.png" alt="二分类代价矩阵"></p>
<p>注：<script type="math/tex">cost_{ii}=0</script>表示分类正确的代价为0</p>
<p><strong>代价敏感错误率（cost-sesitive error rate）</strong>：</p>
<ul>
<li>离散：</li>
</ul>
<script type="math/tex; mode=display">
E(f;D;cost)=\frac{1}{m}\Big(\sum_{x_i\in D^+}\amalg(f(x_i)\neq y_i)\times cost_{01}+\sum_{x_i\in D^-}\amalg(f(x_i)\neq y_i)\times cost_{10}\Big)</script><ul>
<li>连续：</li>
</ul>
<script type="math/tex; mode=display">
E(f;D;cost)=cost_{01}\int_{x\sim D^+}{\amalg(f(x)\neq y)p(x)dx}+cost_{10}\int_{x\sim D^-}{\amalg(f(x)\neq y)p(x)dx}</script><p><strong>代价曲线（cost curve）</strong>：</p>
<p>p是样例为正例的概率，则</p>
<p>横轴为正例概率代价：<script type="math/tex">P(+)cost=\frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}</script></p>
<p>纵轴为归一化代价：<script type="math/tex">cost_{norm}=\frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}</script></p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/代价曲线和期望总体代价.png" alt="代价曲线和期望总体代价"></p>
<p>注：ROC曲线上的每一点对应了代价平面上的一条线段，取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价。</p>
<h1 id="2-4-比较检验"><a href="#2-4-比较检验" class="headerlink" title="2.4 比较检验"></a>2.4 比较检验</h1><p><strong>统计假设检验（hypothesis test）</strong>：验证所选模型在结构上、形式上、变化方向上能否代表客观情况。若在测试集上观察到学习器A比B好，那么基于假设检验的结果我们可以推断出A的泛化能力是否在统计意义上优于B，以及这个结论的把握有多大。</p>
<h2 id="2-4-1-假设检验"><a href="#2-4-1-假设检验" class="headerlink" title="2.4.1 假设检验"></a>2.4.1 假设检验</h2><blockquote>
<p><strong>假设检验五步骤：</strong></p>
<p>Step1：根据实际问题要求提出原假设和备选假设</p>
<p>Step2：选择一个合适的检验统计量，并确定拒绝域</p>
<p>Step3：给定显著性水平<script type="math/tex">\alpha</script></p>
<p>Step4：计算在该显著性水平下、原假设成立条件下检验统计量的具体值</p>
<p>Step5：做出判断，若计算结果在拒绝域内则拒绝原假设，备选假设成立；否则原假设成立</p>
</blockquote>
<p><em>具体参见：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42468475/article/details/116240528?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168597517916800180686762%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=168597517916800180686762&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-116240528-null-null.142^v88^insert_down38v5,239^v2^insert_chatgpt&amp;utm_term=假设检验&amp;spm=1018.2226.3001.4187">假设检验及例题讲解</a></em></p>
<h2 id="2-4-2-交叉验证t检验"><a href="#2-4-2-交叉验证t检验" class="headerlink" title="2.4.2 交叉验证t检验"></a>2.4.2 交叉验证t检验</h2><p><strong>k折交叉验证成对t检验（k-fold cross validation paired t-test）</strong>：对两个学习器A和B采用相同的k折交叉验证法，得到成对的测试错误率<script type="math/tex">\epsilon_1^A,\epsilon_2^A,...,\epsilon_k^A</script>和<script type="math/tex">\epsilon_1^B,\epsilon_2^B,...,\epsilon_k^B</script>，再对每对结果求差<script type="math/tex">\Delta_i=\epsilon_i^A+\epsilon_i^B</script>，最后求出差值<script type="math/tex">\Delta_1,\Delta_2,...,\Delta_k</script>的均值和方差，在显著性水平<script type="math/tex">\alpha</script>下，若变量<script type="math/tex">\tau_t=\lvert\sqrt{k}\mu/\sigma\rvert</script>小于临界值小于<script type="math/tex">t_{\alpha/2,k-1}</script>，则假设不能被拒绝，即认为两个学习器的性能相同；否则拒绝假设，即认为两个学习器的性能不同，且平均错误率较小的那个学习器性能更好。</p>
<p>注：通常采用<script type="math/tex">5\times 2</script>交叉验证法</p>
<h2 id="2-4-3-McNemar检验"><a href="#2-4-3-McNemar检验" class="headerlink" title="2.4.3 McNemar检验"></a>2.4.3 McNemar检验</h2><p><strong>列联表（contingency table）</strong>：</p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/两个学习器分类差别的列联表.png" alt="两个学习器分类差别的列联表"></p>
<p><strong>McNemar检验</strong>：考虑变量</p>
<script type="math/tex; mode=display">
\tau_{\chi^2}=\frac{(\lvert e_{01}-e_{10}\rvert -1)^2}{e_{01}+e_{10}}</script><p>服从自由度为1的卡方分布。给定显著度<script type="math/tex">\alpha</script>，若该变量值小于临界值<script type="math/tex">\chi_{\alpha}^2</script>则不能拒绝假设，即认为两个学习器的性能相同；否则拒绝假设，即认为两个学习器的性能不同，且平均错误率较小的那个学习器性能更好。</p>
<h2 id="2-4-4-Friedman检验-与-Nemenyi后续检验"><a href="#2-4-4-Friedman检验-与-Nemenyi后续检验" class="headerlink" title="2.4.4 Friedman检验 与 Nemenyi后续检验"></a>2.4.4 Friedman检验 与 Nemenyi后续检验</h2><blockquote>
<p>当有多个算法参与比较时，一种做法是在每个数据集上分别列出两两比较的结果，而在两两比较时可以采用前述方法；另一种方法更直接，即使用基于算法排序的Friedman检验。</p>
</blockquote>
<p><strong>Friedman检验</strong>：在N个数据集上对k个算法进行比较，首先使用留出法/交叉验证法得到每个算法在每个数据集上的测试结果，然后在每个数据集上根据测试性能由好到坏进行排序并赋序值1, 2, …（若性能相同则评分序值），之后求出每个算法的平均序值。设第i个算法的平均序值为<script type="math/tex">r_i</script>，其均值和方差分别为<script type="math/tex">(k+1)/2</script>和<script type="math/tex">(k^2-1)/12N</script>，考虑变量</p>
<script type="math/tex; mode=display">
\tau_F=\frac{(N-1)\tau_{\chi^2}}{N(k-2)-\tau_{\chi^2}}</script><script type="math/tex; mode=display">
\tau_{\chi^2} = \frac{k-1}{k}\times\frac{12N}{k^2-1}\sum_{i=1}^{k}{\Big(r_i-\frac{k+1}{2}\Big)^2}=\frac{12N}{k(k+1)}{\sum_{i=1}^{k}{\Big(r_i^2}-\frac{k(k+1)^2}{4}\Big)}</script><p>服从自由度为k-1和(k-1)(N-1)的F分布。</p>
<blockquote>
<p>若“所有算法性能相同”的假设被拒绝，则说明算法的性能显著不同，这时需要进行“后续检验”（post-hoc test）来进一步区分各个算法，常用的有Nemeyi后续检验。</p>
</blockquote>
<p><strong>Nemeyi后续检验</strong>：计算出平均序值差别的临界值域</p>
<script type="math/tex; mode=display">
CD=q_\alpha\sqrt{\frac{k(k+1)}{6N}}</script><p>若两个算法的平均序值之差超过了临界值域，则认为两个算法的性能显著不同；否则则认为两个算法的性能没有显著差别。</p>
<h1 id="2-5-偏差和方差"><a href="#2-5-偏差和方差" class="headerlink" title="2.5 偏差和方差"></a>2.5 偏差和方差</h1><p><strong>偏差-方差分解（bias-variance decomposition）</strong>：解释学习算法泛化性能的重要工具，试图对学习算法的期望泛化错误率进行拆解。</p>
<p><strong>方差（variance）</strong>：模型多次输出和平均值之间的差的期望（反映模型稳定性）</p>
<script type="math/tex; mode=display">
var(x)=E_D\Big[(f(x;D)-\bar f(x))^2\Big]</script><p><strong>偏差（bias）</strong>：期望输出与真实标记的差别（反映模型本身拟合能力）</p>
<script type="math/tex; mode=display">
bias^2(x)=(\bar f(x)-y)^2</script><p><strong>噪声</strong>：</p>
<script type="math/tex; mode=display">
\epsilon^2=E_D\Big[(y_D-y)^2\Big]</script><p><strong>泛化误差</strong>：可以分解为偏差、方差与噪声之和</p>
<script type="math/tex; mode=display">
E(f;D)=bias^2(x)+var(x)+\epsilon^2</script><p><strong>偏差-方差窘境（bias-variance dilemma）</strong>：偏差和方差是有冲突的</p>
<p><img src="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/泛化误差与偏差、方差的关系.png" alt="泛化误差与偏差、方差的关系"></p>
<p>注：训练程度低时，模型出现欠拟合；训练程度高时，模型出现过拟合。关键在于找到泛化误差曲线的最小值点，该点所对应的训练程度为最优。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%AA%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nashswift">
      <meta itemprop="description" content="What matters is not your ability, but your choice.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长野原烟花店">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%AA%E8%AE%BA/" class="post-title-link" itemprop="url">《机器学习》第一章 绪论</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-05 00:00:00 / 修改时间：23:44:13" itemprop="dateCreated datePublished" datetime="2023-06-05T00:00:00+08:00">2023-06-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-study/" itemprop="url" rel="index"><span itemprop="name">ML study</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h1><p><strong>机器学习</strong>：致力于研究如何通过计算的手段，利用经验（数据）来改善系统自身的性能，是研究关于在计算机上从数据中产生“模型”（model）的算法，即“学习算法”。<br><strong>模型</strong>：泛指从数据中学得的结果，一些文献中用“模型”指全局性结果，而用“模式”指局部性结果。</p>
<h1 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h1><p><strong>数据集（data set）</strong>：一组记录的集合。<br><strong>样本（sample）</strong>：关于一个事件或对象的描述（记录）。<br><strong>特征（feature）&#x2F;属性（attribute）</strong>：反映事件或对象在某一方面的表现或性质。<br><strong>属性值（attribute）</strong>：属性上的取值。<br><strong>属性空间（attribute space）&#x2F;样本空间（sample space）&#x2F;输入空间（input space）</strong>：属性张成的空间。<br><strong>特征向量（feature vector）</strong>：属性空间中一个点对应的一个坐标向量，这里的点就是一个示例。<br><strong>标记&#x2F;标签（label）</strong>：关于示例结果的信息。<br><strong>样例（example）</strong>：拥有了标记信息的示例。<br><strong>训练（training）</strong>：从数据中学得模型。<br><strong>测试（testing）</strong>：学得模型后，使用其进行预测。</p>
<p><em><strong>监督学习（supervised learning）：</strong></em><br>分类（classification）：预测的是离散值。<br>回归（regression）：预测的是连续值。<br><em><strong>无监督学习（unsupervised learning）：</strong></em><br>聚类（clustering）：将不同样本划分为若干簇，需注意在学习过程中我们使用的训练样本通常不拥有标记信息且不清楚所划分的簇的具体概念。</p>
<p><strong>泛化（generalization）能力</strong>：学得模型适用于新样本的能力。<br><strong>补充：</strong><br>经验误差（empirical error）&#x2F;训练误差（training error）：模型在训练集上的误差。<br>泛化误差（generalization error）：模型在新样本集（测试集）上的误差。</p>
<h1 id="1-3-假设空间"><a href="#1-3-假设空间" class="headerlink" title="1.3 假设空间"></a>1.3 假设空间</h1><p><strong>归纳(induction)<strong>和</strong>演绎（deduction）</strong>是科学推理的两大基本手段，前者是从特殊到一般的“泛化”过程，后者是从一般到特殊的“特化”过程。<br>显然，“从样例中学习”是一个归纳的过程，因此亦称为“归纳学习”（inductive learning）。</p>
<p>我们可以把学习过程看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配（fit）的假设。现实问题中，我们常常面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因此可能有多个假设与训练集一致，即存在着一个与训练集一致的假设集合，我们称之为版本空间（version space）。</p>
<h1 id="1-4-归纳偏好"><a href="#1-4-归纳偏好" class="headerlink" title="1.4 归纳偏好"></a>1.4 归纳偏好</h1><p><strong>归纳偏好（inductive bias）</strong>：机器学习算法在学习过程中对某种类型假设的偏好，任何一个有效的机器学习算法必有其归纳偏好，算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。<br><strong>奥卡姆剃刀（Occam’s razor）原理</strong>：若有多个假设与观察一致，则选最简单的那个（简单有效原则）。<br><strong>没有免费的午餐定理（NFL定理）</strong>：<br>前提：所有问题出现的机会相同、或所有问题同等重要。<br>内容：无论学习算法a多聪明、学习算法b多笨拙，它们的期望性能相同。如果某种算法在某些方面比另一种算法更优，那就必然在其他某些方面弱于另一种算法。<br>启示：要谈论算法的相对优劣，必须要针对具体的学习问题。</p>
<h1 id="1-5-发展历程"><a href="#1-5-发展历程" class="headerlink" title="1.5 发展历程"></a>1.5 发展历程</h1><h1 id="1-6-应用现状"><a href="#1-6-应用现状" class="headerlink" title="1.6 应用现状"></a>1.6 应用现状</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/04/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E8%AF%B4%E6%98%8E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nashswift">
      <meta itemprop="description" content="What matters is not your ability, but your choice.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长野原烟花店">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/04/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E8%AF%B4%E6%98%8E/" class="post-title-link" itemprop="url">个人博客的说明</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-04 14:53:53" itemprop="dateCreated datePublished" datetime="2023-06-04T14:53:53+08:00">2023-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-05 23:43:37" itemprop="dateModified" datetime="2023-06-05T23:43:37+08:00">2023-06-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="个人博客的说明"><a href="#个人博客的说明" class="headerlink" title="个人博客的说明"></a>个人博客的说明</h1><h3 id="一些废话"><a href="#一些废话" class="headerlink" title="一些废话"></a>一些废话</h3><p>本博客由Nash Swift于公元2023年6月4日，在福州市家中创建，其主要目的是记录本人在从事人工智能相关方向的学习和工作中的点滴琐碎。如果在将来的某一天，我的博客帮助到了某位朋友，在此提前说一声不客气^_^。如果需要联系本人，请发邮件至<a href="mailto:&#55;&#48;&#x39;&#x32;&#x32;&#x32;&#50;&#53;&#x31;&#x40;&#x71;&#113;&#46;&#99;&#x6f;&#109;">&#55;&#48;&#x39;&#x32;&#x32;&#x32;&#50;&#53;&#x31;&#x40;&#x71;&#113;&#46;&#99;&#x6f;&#109;</a>，我会及时地回复。由于这是本人第一次搭建个人博客，因此仍有许多不足之处，也以此勉励自己在未来的生活中不断学习精进，不断完善自身。</p>
<h3 id="特别鸣谢"><a href="#特别鸣谢" class="headerlink" title="特别鸣谢"></a>特别鸣谢</h3><p>本人在此特别鸣谢我的父亲和母亲，你们对我的关心和爱让我成为了更加优秀的自己；感谢我家的泰迪狗（名：逗逗），你的可爱与陪伴温暖了我的家庭和我的心；感谢福州华伦中学2015届全体师生、感谢福州第一中学2018届全体师生、感谢中南大学机电工程学院机械设计制造及自动化专业2022届全体师生，以及在我漫漫求学之路上帮助过我的每一个人，你们见证了我过去的足迹，而我也将带着你们希冀的目光不断前行；感谢一位名为Mandy的可爱女孩，你陪伴我经历了最为宝贵的10年青春，是我最好最好最好的朋友，友谊地久天长！最后也提前感谢厦门大学信息学院人工智能研究院的各位老师，感谢smartdsp实验室的各位师生，在未来三年的研究生阶段能与诸位同行，是我极大的荣幸。<br>鄙人一介平民，才疏学浅，但却有着满腔热诚，希望未来的自己能始终坚定地走着自己的道路，不忘初心方得始终。<br>最后借用《红楼梦》中薛宝钗所作《临江仙·柳絮》中的一句话：好风凭借力，送我上青云！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/04/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nashswift">
      <meta itemprop="description" content="What matters is not your ability, but your choice.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长野原烟花店">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/04/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-04 00:01:20" itemprop="dateCreated datePublished" datetime="2023-06-04T00:01:20+08:00">2023-06-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="nashswift"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">nashswift</p>
  <div class="site-description" itemprop="description">What matters is not your ability, but your choice.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-06 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nashswift</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
