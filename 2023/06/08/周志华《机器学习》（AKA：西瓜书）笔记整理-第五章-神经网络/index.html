<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="5.1 神经元模型神经网络（neural network）：是一个强大的机器学习方法，由具有适应性的简单单元——神经元（neuron）组成的广泛并行互连的网络，能够模拟生物神经系统对真实世界物体作出交互式反应，是深度学习（deep learning）的基础。 M-P神经元模型：神经元接收到其他n个神经元传递进来的输入信号，这些输入信号通过带权重的连接（connection）进行传递，并将收到的总输">
<meta property="og:type" content="article">
<meta property="og:title" content="周志华《机器学习》（AKA：西瓜书）笔记整理-第五章-神经网络">
<meta property="og:url" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="长野原烟花店">
<meta property="og:description" content="5.1 神经元模型神经网络（neural network）：是一个强大的机器学习方法，由具有适应性的简单单元——神经元（neuron）组成的广泛并行互连的网络，能够模拟生物神经系统对真实世界物体作出交互式反应，是深度学习（deep learning）的基础。 M-P神经元模型：神经元接收到其他n个神经元传递进来的输入信号，这些输入信号通过带权重的连接（connection）进行传递，并将收到的总输">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/M-P%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%B8%E5%9E%8B%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%84%9F%E7%9F%A5%E6%9C%BA.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%A4%9A%E5%B1%82%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP%E7%BD%91%E7%BB%9C.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A0%87%E5%87%86BP%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%A8%E5%B1%80%E6%9C%80%E5%B0%8F%E4%B8%8E%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/SOM.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CCL.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Elman.png">
<meta property="og:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Boltzmann%E6%9C%BA%E5%92%8CRBM.png">
<meta property="article:published_time" content="2023-06-08T14:00:37.000Z">
<meta property="article:modified_time" content="2023-06-08T16:36:55.350Z">
<meta property="article:author" content="nashswift">
<meta property="article:tag" content="Machine Learning, Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/M-P%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.png">

<link rel="canonical" href="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>周志华《机器学习》（AKA：西瓜书）笔记整理-第五章-神经网络 | 长野原烟花店</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">长野原烟花店</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nashswift">
      <meta itemprop="description" content="What matters is not your ability, but your choice.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长野原烟花店">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          周志华《机器学习》（AKA：西瓜书）笔记整理-第五章-神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-06-08 22:00:37" itemprop="dateCreated datePublished" datetime="2023-06-08T22:00:37+08:00">2023-06-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-09 00:36:55" itemprop="dateModified" datetime="2023-06-09T00:36:55+08:00">2023-06-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-study/" itemprop="url" rel="index"><span itemprop="name">ML study</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="5-1-神经元模型"><a href="#5-1-神经元模型" class="headerlink" title="5.1 神经元模型"></a>5.1 神经元模型</h1><p><strong>神经网络（neural network）</strong>：是一个强大的机器学习方法，由具有适应性的简单单元——<strong>神经元（neuron）</strong>组成的广泛并行互连的网络，能够模拟生物神经系统对真实世界物体作出交互式反应，是<strong>深度学习（deep learning）</strong>的基础。</p>
<p><strong>M-P神经元模型</strong>：神经元接收到其他n个神经元传递进来的输入信号，这些输入信号通过带权重的连接（connection）进行传递，并将收到的总输入值与神经元阈值相比较，最后通过<strong>激活函数（active function）</strong>处理以产生神经元的输出。把许多这样的神经元按照一定的层次结构连接起来，就得到了神经网络。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/M-P神经元模型.png" alt="M-P神经元模型"></p>
<p><strong>激活函数（active function）</strong>：常见的激活函数有</p>
<ul>
<li><strong>单位阶跃函数</strong>：将输入值映射为0（神经元抑制），1（神经元兴奋）。但该函数具有<strong>不连续、不光滑</strong>等不好的性质，不便于后续求导。</li>
<li><strong>对率函数（logistic function）/挤压函数（squashing function）</strong>：是一种<strong>“Sigmoid”函数</strong>，将较大范围内变化的输入值挤压到(0,1)输出值范围内，且函数处处可导，因此更为常用。</li>
</ul>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/典型的神经元激活函数.png" alt="典型的神经元激活函数"></p>
<h1 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="5.2 感知机与多层网络"></a>5.2 感知机与多层网络</h1><h2 id="5-2-1-感知机"><a href="#5-2-1-感知机" class="headerlink" title="5.2.1 感知机"></a>5.2.1 感知机</h2><p><strong>感知机（perceptron）</strong>：由两层神经元组成，输入层接受外界输入信号后传递给输出层，输出层是M-P神经元（也称为阈值逻辑单元）。注意到<script type="math/tex">y=f(\sum_iw_ix_i-\theta)</script>，将阈值<script type="math/tex">\theta</script>看作输入为-1的哑节点（dummy node），对应的连接权重为<script type="math/tex">w_{n+1}</script>，这样权重和阈值的学习就可以统一为权重的学习。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/感知机.png" alt="感知机"></p>
<p><strong>感知机的学习规则</strong>：利用梯度下降法进行调整</p>
<script type="math/tex; mode=display">
\begin{split}
w_i \leftarrow w_i+\Delta w_i\\
\Delta w_i=\eta(y-\hat{y})x_i
\end{split}</script><blockquote>
<p>感知机只有输出层神经元进行激活函数处理，即只有一层功能神经元（functional neuron），其学习能力非常有限，只能处理线性可分的为题，不能解决诸如异或问题这样的非线性可分问题。</p>
</blockquote>
<h2 id="5-2-2-多层功能神经元（多层网络）"><a href="#5-2-2-多层功能神经元（多层网络）" class="headerlink" title="5.2.2 多层功能神经元（多层网络）"></a>5.2.2 多层功能神经元（多层网络）</h2><p>对于线性不可分的问题（如异或问题等），则需要使用多层功能神经元。输出层与输入层之间的一层神经元，被称为<strong>隐层（hidden layer）</strong>，隐层和输出层神经元都是拥有激活函数的功能神经元。</p>
<p><strong>多层前馈神经网络（multi-layer feedforward neural networks）</strong>：每层神经元与下层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，前馈是指网络拓扑结构上不存在环或者回路。</p>
<ul>
<li>输出层：输出层神经元为功能神经元，进行函数处理。</li>
<li>隐层：隐层神经元为功能神经元，进行函数处理。</li>
<li>输入层：输入层神经元仅接收外界输入信息，不进行函数处理。</li>
</ul>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/多层前馈神经网络.png" alt="多层前馈神经网络"></p>
<blockquote>
<p>神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权重和神经元阈值。换言之，神经网络“学习”到的东西，蕴含在连接权重和阈值中。</p>
</blockquote>
<h1 id="5-3-误差逆传播算法（BP算法）——对梯度计算结果的解释"><a href="#5-3-误差逆传播算法（BP算法）——对梯度计算结果的解释" class="headerlink" title="5.3 误差逆传播算法（BP算法）——对梯度计算结果的解释"></a>5.3 误差逆传播算法（BP算法）——对梯度计算结果的解释</h1><h2 id="5-3-1-标准BP算法"><a href="#5-3-1-标准BP算法" class="headerlink" title="5.3.1 标准BP算法"></a>5.3.1 标准BP算法</h2><p><strong>误差逆传播（error BackPropagation，简称BP）算法</strong>：给定训练集<script type="math/tex">D=\{ (x_1,y_1),...,(x_m,y_m)\},x_i\in\Re^d,y_i\in\Re^l</script>，即输入样本有d个属性，输入向量为l维。假定多层前馈神经网络拥有d个输入神经元，l个输出神经元，q个隐层神经元，其中具体参数有：</p>
<ul>
<li><script type="math/tex">\theta_j</script>：输出层第j个神经元的阈值</li>
<li><script type="math/tex">\gamma_h</script>：隐层第h个神经元的阈值</li>
<li><script type="math/tex">\upsilon_{ih}</script>：输入层第i个神经元与隐层第h个神经元之间的连接权重</li>
<li><script type="math/tex">\omega_{hj}</script>：隐层第h个神经元与输出层第j个神经元之间的连接权重</li>
<li><script type="math/tex">x_i</script>：输入层第i个神经元的输入</li>
<li><script type="math/tex">y_j</script>：输出层第j个神经元的输出</li>
<li><script type="math/tex">\alpha_h</script>；隐层第h个神经元的输入</li>
<li><script type="math/tex">b_h</script>：隐层第h个神经元的输出</li>
<li><script type="math/tex">\beta_j</script>：输出层第j个神经元的输入</li>
</ul>
<p>假设功能神经元激活函数都为对率函数<script type="math/tex">y=\frac{1}{1+e^{-x}}</script>。该多层前馈神经网络中总共有(d+1+l)*q+l个参数需要确定，BP是一个迭代学习算法，在迭代的每一轮都采用广义感知机学习规则（基于梯度下降策略寻找最小化均方误差，以目标的负梯度方向对参数进行调整）对参数进行更新估计，因此对于任意一个参数v，更新公式为<script type="math/tex">v \leftarrow v+\Delta v</script>。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP网络.png" alt="BP网络"></p>
<p>BP算法中的更新公式：</p>
<script type="math/tex; mode=display">
\Delta w_{hj}=\eta g_jb_h</script><script type="math/tex; mode=display">
\Delta \theta_j=-\eta g_j</script><script type="math/tex; mode=display">
\Delta \upsilon_{ih}=\eta e_hx_i</script><script type="math/tex; mode=display">
\Delta \gamma_h=-\eta e_h</script><p>其中，<script type="math/tex">g_i</script>为输出层神经元梯度项，<script type="math/tex">e_h</script>为隐层神经元梯度项，具体值如下：</p>
<script type="math/tex; mode=display">
g_j=\hat{y_j}^k(1-\hat{y_j}^k)(y_j^k-\hat{y_j}^k)</script><script type="math/tex; mode=display">
e_h=b_h(1-b_h)\sum_{j=1}^{l}w_{hj}g_j</script><blockquote>
<p>学习率<script type="math/tex">\eta\in(0,1)</script>控制着算法每一轮迭代中更新的步长（一般设置为0.1）：</p>
<ul>
<li><script type="math/tex">\eta</script>太大则容易震荡</li>
<li><script type="math/tex">\eta</script>太小则收敛速度过慢</li>
</ul>
</blockquote>
<p><strong>标准BP算法的工作流程</strong>：</p>
<ol>
<li><p>前向传播过程：输入示例传给输入层，再逐层将信号前传直到产生输出层结果；</p>
</li>
<li><p>反向传播过程：计算输出层输出误差（line4-5），再将误差逆传播至隐层神经元（line6），最后根据隐层神经元的误差来对连接权重和阈值进行调整（line7）。</p>
</li>
</ol>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/标准BP算法流程.png" alt="标准BP算法流程"></p>
<h2 id="5-3-2-累积BP算法"><a href="#5-3-2-累积BP算法" class="headerlink" title="5.3.2 累积BP算法"></a>5.3.2 累积BP算法</h2><p>由于BP算法的目标是最小化训练集D上的累积误差<script type="math/tex">E=\frac{1}{m}\sum_{k=1}^{m}E_k</script>，标准BP算法的更新规则是基于for循环下的单个<script type="math/tex">E_k</script>推导而得到的，参数更新非常频繁且迭代次数多。若直接针对累积误差进行最小化，在读取训练集D一遍（一轮学习）后才进行参数更新，就得到了<strong>累积误差逆传播（accumulated error backpropagation）算法</strong>。</p>
<blockquote>
<p>注：标准BP算法和累积BP算法的区别类似于随机梯度下降与标准梯度下降的区别。</p>
</blockquote>
<p>累积BP算法参数更新的频率相对标准BP算法而言低很多。但在很多任务中，累积误差下降到一定程度后改变非常缓慢，这时标准BP算法往往会更快获得较好的解，在训练集D很大时尤其明显；如果数据集中的数据点之间的一致性较好，则采用累积BP算法往往更优。</p>
<h2 id="5-3-3-BP神经网络中的两个问题"><a href="#5-3-3-BP神经网络中的两个问题" class="headerlink" title="5.3.3 BP神经网络中的两个问题"></a>5.3.3 BP神经网络中的两个问题</h2><ul>
<li><strong>问题一：如何设置隐层神经元个数？</strong></li>
</ul>
<p><strong>通用近似定理/万能近似定理</strong>：只需一个包含足够多神经元的隐层，多层前馈神经网络能够以任意精度逼近任意复杂度的连续函数，该定理指出了人工神经网络近似任意模型（函数）的能力。</p>
<p>但如何设置隐层神经元个数仍是<strong>未决问题</strong>，实际运用中通常采用<strong>“试错法”（trail-by-error）</strong>进行调整。</p>
<ul>
<li><strong>问题二：如何解决过拟合问题？</strong></li>
</ul>
<p>由于BP神经网络的强大表示能力，因此其经常遭遇过拟合，其训练误差持续降低而测试误差仍在上升，常用的两种缓解策略如下：</p>
<ol>
<li><strong>早停法（early stopping）</strong>：将数据划分为训练集和验证集（validation set），训练集用来计算梯度、更新连接权重和阈值，验证集用来估计误差。若训练集误差不断降低但是验证集误差开始升高，则意味着此时出现过拟合，应停止训练，同时返回具有最小验证集误差的参数（连接权重和阈值）。</li>
<li><strong>正则化（regularization）</strong>：其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，来限制模型的拟合能力。</li>
</ol>
<h1 id="5-4-全局最小和局部极小"><a href="#5-4-全局最小和局部极小" class="headerlink" title="5.4 全局最小和局部极小"></a>5.4 全局最小和局部极小</h1><p><strong>局部极小值陷阱</strong>：由于采用梯度下降法进行参数寻优，若误差函数在当前点的梯度为0则不再更新，此时达到了局部极小，这就意味着参数的迭代更新到此为止。但如果误差函数有多个局部极小，那么就不能保证当前找到的参数解是全局最小（最优），这就是局部极小值陷阱。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/全局最小与局部极小.png" alt="全局最小与局部极小"></p>
<p><strong>“跳出”局部极小值的方法</strong>：</p>
<ul>
<li>采用多组不同参数值初始化多个神经网络，训练后取其中误差最小的解作为最优参数。</li>
<li>采用模拟退火（simulated annealing）技术，在每一次迭代中都有一定概率接受“次优解”（概率要逐渐降低），从而有助于跳出局部极小。</li>
<li>使用随机梯度下降，引入随机因素。</li>
<li>使用遗传算法（genetic algorithms）来更好地逼近全局最小。</li>
</ul>
<blockquote>
<p>以上方法大多是启发式，理论上尚缺乏保障。</p>
</blockquote>
<h1 id="5-5-其他神经网络（简介）"><a href="#5-5-其他神经网络（简介）" class="headerlink" title="5.5 其他神经网络（简介）"></a>5.5 其他神经网络（简介）</h1><h2 id="5-5-1-径向基函数（RBF）网络"><a href="#5-5-1-径向基函数（RBF）网络" class="headerlink" title="5.5.1 径向基函数（RBF）网络"></a>5.5.1 径向基函数（RBF）网络</h2><p><strong>RBF网络</strong>：是一种单隐层前馈神经网络，并使用<strong>径向基函数</strong>（如高斯径向基函数<script type="math/tex">\rho(x,c_i)=e^{-\beta_i||x-c_i||^2}</script>）作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。</p>
<p><strong>RBF网络训练过程</strong>：</p>
<ol>
<li>确定神经元中心（如聚类、随机采样等）</li>
<li>利用BP算法确定参数</li>
</ol>
<h2 id="5-5-2-自适应谐振理论（ART）网络"><a href="#5-5-2-自适应谐振理论（ART）网络" class="headerlink" title="5.5.2 自适应谐振理论（ART）网络"></a>5.5.2 自适应谐振理论（ART）网络</h2><p><strong>竞争型学习（competitive learning）</strong>：是神经网络中一种无监督学习策略，网络的输出神经元相互竞争，同一时刻只有一个胜者神经元被激活，其他被抑制【“胜者通吃”（winner-take-all）原则】。</p>
<p><strong>ART网络</strong>：主要思想是竞争型学习。该网络由比较层、识别层、识别阈值和重置模块构成.比较层负责接收输入样本，并将其传递给识别层神经元.识别层每个神经元对应 一个模式类，<strong>神经元数目可在训练过程中动态增长</strong>以增加新的模式类。</p>
<p><strong>ART网络优点</strong>：</p>
<ol>
<li>较好地缓解了竞争型学习中“可塑性-稳定性窘境”（stability plasticity dilemma）</li>
<li>可进行增量学习（incremental learning）或在线学习（online learning）</li>
</ol>
<h2 id="5-5-3-自组织映射（SOM）网络"><a href="#5-5-3-自组织映射（SOM）网络" class="headerlink" title="5.5.3 自组织映射（SOM）网络"></a>5.5.3 自组织映射（SOM）网络</h2><p><strong>SOM网络</strong>：是一种竞争学习型无监督神经网络。它能将<strong>高维输入数据映射到低维</strong>，同时保持其在高维空间中的拓扑结构。因此SOM的训练目标就是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的。</p>
<p><strong>SOM运用</strong>：聚类、高维数据可视化、图像分割等方面均有广泛运用。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/SOM.png" alt="SOM"></p>
<h2 id="5-5-4-级联相关（CCL）网络"><a href="#5-5-4-级联相关（CCL）网络" class="headerlink" title="5.5.4 级联相关（CCL）网络"></a>5.5.4 级联相关（CCL）网络</h2><p><strong>CCL网络</strong>：是结构适应网络（构造性神经网络）的代表，将网络结构也当作学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CCL.png" alt="CCL"></p>
<p><strong>CCL网络中两个主要部分</strong>：</p>
<ol>
<li>“级联”：建立层次连接的层级结构（确定网络结构）</li>
<li>“相关”：通过最大化新神经元的输出与网络误差之间的相关性来训练参数（训练参数）</li>
</ol>
<blockquote>
<p>级联相关网络不需要设定网络层数和隐层神经元数量，且训练速度快，但在数据较小时易陷入过拟合。</p>
</blockquote>
<h2 id="5-5-5-Elman网络"><a href="#5-5-5-Elman网络" class="headerlink" title="5.5.5 Elman网络"></a>5.5.5 Elman网络</h2><p><strong>Elman网络</strong>：是递归神经网络（recurrent neural networks）的一种，允许网络中出现环形结构。它的结构与多层前馈网络相似，但隐层神经元输出被反馈回来称为神经元下一时刻输入的一部分。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Elman.png" alt="Elman"></p>
<h2 id="5-5-6-Boltzmann机"><a href="#5-5-6-Boltzmann机" class="headerlink" title="5.5.6 Boltzmann机"></a>5.5.6 Boltzmann机</h2><p><strong>Boltzmann机</strong>：是一种“基于能量的模型”（energy-based model），为网络状态定义一个Boltzmann机能量，网络的训练就是在最小化这个能量函数使得网络达到理想状态——Boltzmann分布。现实中常采用的是受限的Boltzmann机（Restricted Boltzmann Machine，简称RBM）。</p>
<p><img src="/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Boltzmann机和RBM.png" alt="Boltzmann机和RBM"></p>
<h1 id="5-6-深度学习"><a href="#5-6-深度学习" class="headerlink" title="5.6 深度学习"></a>5.6 深度学习</h1><p><strong>深度学习（deep learning） = 表示学习（representation learning）/特征学习（feature learning） + 机器学习</strong></p>

    </div>

    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      </div>
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>nashswift
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/06/08/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="周志华《机器学习》（AKA：西瓜书）笔记整理-第五章-神经网络">http://example.com/2023/06/08/周志华《机器学习》（AKA：西瓜书）笔记整理-第五章-神经网络/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning-Deep-Learning/" rel="tag"># Machine Learning, Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/06/07/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91/" rel="prev" title="周志华《机器学习》（AKA：西瓜书）笔记整理-第四章-决策树">
      <i class="fa fa-chevron-left"></i> 周志华《机器学习》（AKA：西瓜书）笔记整理-第四章-决策树
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/06/09/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E5%85%AD%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="next" title="周志华《机器学习》（AKA：西瓜书）笔记整理-第六章-支持向量机">
      周志华《机器学习》（AKA：西瓜书）笔记整理-第六章-支持向量机 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#5-1-%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">5.1 神经元模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-2-%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E5%A4%9A%E5%B1%82%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">5.2 感知机与多层网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-1-%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">2.1.</span> <span class="nav-text">5.2.1 感知机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-2-%E5%A4%9A%E5%B1%82%E5%8A%9F%E8%83%BD%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88%E5%A4%9A%E5%B1%82%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">5.2.2 多层功能神经元（多层网络）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-3-%E8%AF%AF%E5%B7%AE%E9%80%86%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%EF%BC%88BP%E7%AE%97%E6%B3%95%EF%BC%89%E2%80%94%E2%80%94%E5%AF%B9%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-number">3.</span> <span class="nav-text">5.3 误差逆传播算法（BP算法）——对梯度计算结果的解释</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-1-%E6%A0%87%E5%87%86BP%E7%AE%97%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">5.3.1 标准BP算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-2-%E7%B4%AF%E7%A7%AFBP%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">5.3.2 累积BP算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-3-BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E4%B8%A4%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">3.3.</span> <span class="nav-text">5.3.3 BP神经网络中的两个问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-4-%E5%85%A8%E5%B1%80%E6%9C%80%E5%B0%8F%E5%92%8C%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F"><span class="nav-number">4.</span> <span class="nav-text">5.4 全局最小和局部极小</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-5-%E5%85%B6%E4%BB%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E7%AE%80%E4%BB%8B%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">5.5 其他神经网络（简介）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-1-%E5%BE%84%E5%90%91%E5%9F%BA%E5%87%BD%E6%95%B0%EF%BC%88RBF%EF%BC%89%E7%BD%91%E7%BB%9C"><span class="nav-number">5.1.</span> <span class="nav-text">5.5.1 径向基函数（RBF）网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-2-%E8%87%AA%E9%80%82%E5%BA%94%E8%B0%90%E6%8C%AF%E7%90%86%E8%AE%BA%EF%BC%88ART%EF%BC%89%E7%BD%91%E7%BB%9C"><span class="nav-number">5.2.</span> <span class="nav-text">5.5.2 自适应谐振理论（ART）网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-3-%E8%87%AA%E7%BB%84%E7%BB%87%E6%98%A0%E5%B0%84%EF%BC%88SOM%EF%BC%89%E7%BD%91%E7%BB%9C"><span class="nav-number">5.3.</span> <span class="nav-text">5.5.3 自组织映射（SOM）网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-4-%E7%BA%A7%E8%81%94%E7%9B%B8%E5%85%B3%EF%BC%88CCL%EF%BC%89%E7%BD%91%E7%BB%9C"><span class="nav-number">5.4.</span> <span class="nav-text">5.5.4 级联相关（CCL）网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-5-Elman%E7%BD%91%E7%BB%9C"><span class="nav-number">5.5.</span> <span class="nav-text">5.5.5 Elman网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-6-Boltzmann%E6%9C%BA"><span class="nav-number">5.6.</span> <span class="nav-text">5.5.6 Boltzmann机</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-6-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.</span> <span class="nav-text">5.6 深度学习</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="nashswift"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">nashswift</p>
  <div class="site-description" itemprop="description">What matters is not your ability, but your choice.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-06 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nashswift</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共17.6k字</span>
</div>
        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
