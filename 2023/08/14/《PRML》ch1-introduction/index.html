<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1.1. Example: Polynomial Curve Fitting  建模过程（符合一般数据的生成过程：随机函数+随机噪声）： Step1: 从[0, 1]区间上等间距采样，得到输入x对应的标签t Step2: 加上一个高斯噪声——由随机过程&#x2F;数据本身缺失引起  Model使用linear model进行多项式拟合，形式为M阶多项式：  y(x, w) &#x3D; w_0+w_1x+w_2x^2">
<meta property="og:type" content="article">
<meta property="og:title" content="《PRML》ch1_introduction">
<meta property="og:url" content="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/index.html">
<meta property="og:site_name" content="长野原烟花店">
<meta property="og:description" content="1.1. Example: Polynomial Curve Fitting  建模过程（符合一般数据的生成过程：随机函数+随机噪声）： Step1: 从[0, 1]区间上等间距采样，得到输入x对应的标签t Step2: 加上一个高斯噪声——由随机过程&#x2F;数据本身缺失引起  Model使用linear model进行多项式拟合，形式为M阶多项式：  y(x, w) &#x3D; w_0+w_1x+w_2x^2">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88.png">
<meta property="og:image" content="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9.png">
<meta property="og:image" content="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/curve%20fitting%20re-visited.png">
<meta property="og:image" content="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/cross-validation.png">
<meta property="og:image" content="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/loss%20matrix.png">
<meta property="article:published_time" content="2023-08-14T14:50:28.000Z">
<meta property="article:modified_time" content="2023-08-17T06:51:11.084Z">
<meta property="article:author" content="nashswift">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88.png">

<link rel="canonical" href="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>《PRML》ch1_introduction | 长野原烟花店</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">长野原烟花店</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nashswift">
      <meta itemprop="description" content="What matters is not your ability, but your choice.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长野原烟花店">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《PRML》ch1_introduction
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-14 22:50:28" itemprop="dateCreated datePublished" datetime="2023-08-14T22:50:28+08:00">2023-08-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-17 14:51:11" itemprop="dateModified" datetime="2023-08-17T14:51:11+08:00">2023-08-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-study/" itemprop="url" rel="index"><span itemprop="name">ML study</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="1-1-Example-Polynomial-Curve-Fitting"><a href="#1-1-Example-Polynomial-Curve-Fitting" class="headerlink" title="1.1. Example: Polynomial Curve Fitting"></a>1.1. Example: Polynomial Curve Fitting</h1><p><img src="/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/多项式拟合.png" alt="多项式拟合"></p>
<blockquote>
<p><strong>建模过程</strong>（符合一般数据的生成过程：随机函数+随机噪声）：</p>
<p>Step1: 从[0, 1]区间上等间距采样，得到输入x对应的标签t</p>
<p>Step2: 加上一个高斯噪声——由随机过程/数据本身缺失引起</p>
</blockquote>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p><strong>使用linear model进行多项式拟合</strong>，形式为M阶多项式：</p>
<script type="math/tex; mode=display">
y(x, w) = w_0+w_1x+w_2x^2+...+w_Mx^M = \sum_{j=0}^Mw_jx^j</script><p><strong>Notes：</strong>尽管model关于x是non-linear的，但是关于w是linear的，因此称为linear model</p>
<h3 id="Lost-function"><a href="#Lost-function" class="headerlink" title="Lost function"></a>Lost function</h3><p>采用<strong>均方误差SSE</strong>刻画损失：<script type="math/tex">E(w)=\frac{1}{2}\sum_{n=1}^N\{y(x_n,w)-t_n\}^2</script></p>
<p><strong>Notes：</strong></p>
<ol>
<li>当且仅当预测正确（函数图像通过样本点）时，损失为0，否则大于0</li>
<li>1/2仅仅是为了求导方便</li>
</ol>
<h3 id="Model-comparison-Model-selection"><a href="#Model-comparison-Model-selection" class="headerlink" title="Model comparison / Model selection"></a>Model comparison / Model selection</h3><p><strong>选择多项式的阶M</strong>的问题称为model comparison / model selection。</p>
<p><img src="/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/模型选择.png" alt="模型选择"></p>
<p>可以观察到当M取值较大时，容易引发over-fitting，模型会更加迎合那些random noise；当M取值较小时，容易引发under-fitting，模型拟合能力较差。</p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>由于通过<strong>增加样本数</strong>的方法缓解过拟合开销过大，因此引入<strong>regularization</strong>来限制模型的复杂度（控制error和penalty之间的权重），若使用L2 penalty：</p>
<script type="math/tex; mode=display">
\begin{split}
\tilde{E}(w)=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,w)-t_n\}^2+\frac{\lambda}{2}\lVert w\rVert^2 \\
where\quad\lVert w\rVert^2=w^Tw=w_0^2+w_1^2+...+w_M^2
\end{split}</script><p>其中，<script type="math/tex">\lambda</script>称为正则系数，用来控制正则化影响大小（SSE和regularization之间的重要性）的。实际中，通常从training set中分离出一部分称为validation set，用于训练超参<script type="math/tex">(M,\lambda)</script>，但某种程度上是对training set的一种浪费，因此引入后续的贝叶斯统计理论。</p>
<h1 id="1-2-Probability-Theory"><a href="#1-2-Probability-Theory" class="headerlink" title="1.2. Probability Theory"></a>1.2. Probability Theory</h1><h3 id="The-Rules-of-Probability"><a href="#The-Rules-of-Probability" class="headerlink" title="The Rules of Probability"></a>The Rules of Probability</h3><p><strong>离散形式：</strong></p>
<ol>
<li>sum rule: <script type="math/tex">p(X)=\sum_Yp(X,Y)</script></li>
<li>product rule: <script type="math/tex">p(X,Y)=p(Y|X)p(X)</script></li>
</ol>
<p><strong>Notes：</strong>若<script type="math/tex">p(X,Y)=p(X)p(Y)</script>，即<script type="math/tex">p(Y|X)=p(Y)</script>，则称X和Y是<strong>independent</strong></p>
<p><strong>连续形式：</strong></p>
<ol>
<li>sum rule: <script type="math/tex">p(x)=\int p(x, y)dy</script></li>
<li>product rule: <script type="math/tex">p(x,y)=p(y|x)p(x)</script></li>
</ol>
<h3 id="Bayes’s-theorem"><a href="#Bayes’s-theorem" class="headerlink" title="Bayes’s theorem"></a>Bayes’s theorem</h3><script type="math/tex; mode=display">
p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}</script><p>其中，<script type="math/tex">p(X|Y)</script>称为likelihood，<script type="math/tex">p(Y)</script>称为prior probability，<script type="math/tex">p(Y|X)</script>称为posterior probability，由于分母部分<script type="math/tex">p(X)=\sum_Yp(X|Y)p(Y)</script>，因此称为normalization（确保概率归一化后和为1）</p>
<blockquote>
<p>prior probablity是指在没有对X（样本数据）观测时，对Y发生概率的估计，posterior probability 可以视作在观测到X后对prior probability的一种修正。</p>
</blockquote>
<h2 id="1-2-1-Probability-densities"><a href="#1-2-1-Probability-densities" class="headerlink" title="1.2.1 Probability densities"></a>1.2.1 Probability densities</h2><h3 id="Probability-density-function-PDF"><a href="#Probability-density-function-PDF" class="headerlink" title="Probability density function(PDF)"></a>Probability density function(PDF)</h3><script type="math/tex; mode=display">
p(x\in(a,b))=\int_a^bp(x)dx</script><p>而probability density p(x)必须满足两个条件：</p>
<script type="math/tex; mode=display">
\begin{split}
p(x)\geq0 \\
\int_a^bp(x)dx=1
\end{split}</script><h3 id="Cummulative-distribution-function-CDF"><a href="#Cummulative-distribution-function-CDF" class="headerlink" title="Cummulative distribution function(CDF)"></a>Cummulative distribution function(CDF)</h3><script type="math/tex; mode=display">
P(z)=\int_{-\infty}^zp(x)dx</script><p>且满足<script type="math/tex">P'(x)=p(x)</script></p>
<h3 id="Non-linear-change-of-variable（-）"><a href="#Non-linear-change-of-variable（-）" class="headerlink" title="Non-linear change of variable（*）"></a>Non-linear change of variable（*）</h3><p><strong>Notes: </strong>under a nonlinear change of variable, a probability density transforms differently from a simple function, due to the Jacobian factor.</p>
<p>假设随机变量x和y之间存在非线性变换<script type="math/tex">x=g(y)</script>，则x和y的PDF之间存在对应关系：</p>
<script type="math/tex; mode=display">
p_y(y)=p_x(x)\Big\lvert\frac{dx}{dy}\Big\rvert=p_x(g(y))\lvert g'(y)\rvert</script><blockquote>
<p>另一个角度理解：</p>
<p>变量x和y的非线性变换实际上是在CDF上进行的，而由于CDF转为PDF的过程中存在求导，因此上式可视作求导过程的chain rule</p>
</blockquote>
<h2 id="1-2-2-Expectations-and-covariance"><a href="#1-2-2-Expectations-and-covariance" class="headerlink" title="1.2.2 Expectations and covariance"></a>1.2.2 Expectations and covariance</h2><h3 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h3><p><strong>Def: </strong>the average value of some function f(x) under a probability distribution p(x) is called the <em>expectation</em> of f(x) and will be denoted by E[f].</p>
<p><strong>离散形式：</strong><script type="math/tex">E[f]=\sum_xp(x)f(x)</script></p>
<p><strong>连续形式：</strong><script type="math/tex">E[f]=\int p(x)f(x)dx</script></p>
<p>对于概率分布中有给定的有限数量的N个点，由<script type="math/tex">E[f]\simeq\frac{1}{N}\sum_{n=1}^{N}f(x_n)</script>可得，当<script type="math/tex">N\rightarrow\infty</script>时取等，故可以通过采样的方式来估计期望。</p>
<p><strong>Conditional expectation: </strong><script type="math/tex">E_x[f|y]=\sum_xp(x|y)f(x)</script></p>
<h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><p><strong>Def: </strong>the <em>variance</em> of f(x) provides a measure of how much variability there is in f(x) around its mean value E[f(x)].</p>
<script type="math/tex; mode=display">
var[f(x)]=E[f(x)^2]-E[f(x)]^2</script><p><strong>Covariance: </strong>express the extent to which x and y vary together. If x and y are <strong>independent</strong>, then their covariance vanishes.</p>
<script type="math/tex; mode=display">
cov[x, y]=E_{x,y}[xy]-E[x]E[y]</script><h2 id="1-2-3-Bayesian-probabilities"><a href="#1-2-3-Bayesian-probabilities" class="headerlink" title="1.2.3 Bayesian probabilities"></a>1.2.3 Bayesian probabilities</h2><p><strong>统计学派视角：</strong>概率是通过无限可重复的、随机的事件发生频率来刻画的</p>
<p><strong>贝叶斯学派视角：</strong>概率提供了不确定性的定量描述</p>
<p>在观测数据D前，给定模型参数w的prior记作p(w)，而数据观测所带来的影响由likelihood表示，记作p(D|w)，根据Bayesian Theorem：</p>
<script type="math/tex; mode=display">
p(w|D)=\frac{p(D|w)p(w)}{p(D)}</script><p>可得出在观测数据下w的posterior——p(w|D)来刻画w的不确定性，其中p(D)为归一化因子：<script type="math/tex">p(D)=\int p(D|w)p(w)dw</script>。</p>
<p>Bayesian theorem也可以表示为：<script type="math/tex">posterior\propto likelihood \times prior</script></p>
<blockquote>
<p>频率派和贝叶斯派都认为likelihood function具有十分重要的地位：</p>
<ol>
<li>对于频率派而言，认为w是固定的参数而误差来源于观察数据分布和真实数据分布之间的差异，最小化误差函数就等价于MLE，故常使用MLE方法找到能使得likelihood最大的参数w作为模型参数。</li>
<li>对于贝叶斯派而言，认为w由某种概率分布所表达——即先验p(w)，在给定先验p(w)条件下利用Bayesian theorem将MLE转为MAP</li>
</ol>
</blockquote>
<h2 id="1-2-4-The-Gaussian-distribution"><a href="#1-2-4-The-Gaussian-distribution" class="headerlink" title="1.2.4 The Gaussian distribution"></a>1.2.4 The Gaussian distribution</h2><p><strong>一元高斯分布</strong>：</p>
<script type="math/tex; mode=display">
N(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp\bigg\{-\frac{1}{2\sigma^2}(x-\mu)^2\bigg\}</script><p><strong>Notes: </strong><script type="math/tex">\mu</script>称为mean，<script type="math/tex">\sigma^2</script>称为variance，<script type="math/tex">\sigma</script>称为standard deviation，<script type="math/tex">\beta=1/\sigma^2</script>称为precision</p>
<p><strong>多元高斯分布（D维）：</strong></p>
<script type="math/tex; mode=display">
N(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}}\frac{1}{\lvert\Sigma\rvert^{D/2}}\exp\bigg\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg\}</script><p><strong>Notes: </strong><script type="math/tex">\Sigma\in R^{D\times D}</script>称为covariance matrix</p>
<h3 id="MLE-for-Gaussian-distribution"><a href="#MLE-for-Gaussian-distribution" class="headerlink" title="MLE for Gaussian distribution"></a>MLE for Gaussian distribution</h3><p>假设N个观测数据<script type="math/tex">x=(x_1,x_2,...,x_N)^T</script>都是独立从Gaussian distribution中抽样出来的（aka: i.i.d.），则数据的likelihood function可以表示为</p>
<script type="math/tex; mode=display">
p(x|\mu,\sigma^2)=\prod_{n=1}^{N}N(x_n|\mu,\sigma^2)</script><p>通过MLE求参数<script type="math/tex">\mu</script>和<script type="math/tex">\sigma^2</script>得</p>
<script type="math/tex; mode=display">
\begin{split}
\mu_{ML}=\frac{1}{N}\sum_{n=1}^{N}x_n \\
\sigma_{ML}^2=\frac{1}{N}\sum_{n=1}^{N}(x_n-\mu_{ML})^2
\end{split}</script><p>其中对参数<script type="math/tex">\mu</script>的估计是无偏的，但对参数<script type="math/tex">\sigma^2</script>的估计是有偏的（<script type="math/tex">\tilde{\sigma}^2=\frac{N}{N-1}\sigma_{ML}^2</script>），由此可见MLE方法极易导致over-fitting的出现。但当数据量足够大，即<script type="math/tex">N\rightarrow\infty</script>时，<script type="math/tex">\tilde\sigma^2=\sigma_{ML}^2</script>消除了偏差，over-fitting自然也消除了（再次论证了增加数据量能一定程度上消除over-fitting这一结论）。</p>
<h2 id="1-2-5-Curve-fitting-re-visited"><a href="#1-2-5-Curve-fitting-re-visited" class="headerlink" title="1.2.5 Curve fitting re-visited"></a>1.2.5 Curve fitting re-visited</h2><p><strong>Assumption: </strong>给定x，所对应的标签t服从Gaussian distribution且均值为y(x, w)——即分布均值落在拟合曲线上，故likelihood function为</p>
<script type="math/tex; mode=display">
p(t|x,w,\beta)=\prod_{n=1}^{N}N(t_n|y(x_n,w),\beta^{-1})</script><p><img src="/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/curve fitting re-visited.png" alt="curve fitting re-visited"></p>
<p>利用MLE求解参数w和<script type="math/tex">\beta</script>的过程等价于最小化SSE损失，可以得到<script type="math/tex">\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}\{y(x_n,w_{ML})-t_n\}^2</script>。</p>
<p><strong>进一步使用贝叶斯方法</strong>，引入超参数<script type="math/tex">\alpha</script>（precision）控制参数w的先验分布</p>
<script type="math/tex; mode=display">
p(w|\alpha)=N(w|0,\alpha^{-1}I)=\bigg(\frac{\alpha}{2\pi}\bigg)^{(M+1)/2}\exp\bigg\{-\frac{\alpha}{2}w^Tw\bigg\}</script><p>其中M+1表示M阶多项式有M+1个参数，利用贝叶斯公式<script type="math/tex">p(w|x,t,\alpha,\beta)\propto p(t|x,w,\beta)p(w|\alpha)</script>可将MLE转为MAP，而又有</p>
<script type="math/tex; mode=display">
\max\:p(w|x,t,\alpha,\beta) \Leftrightarrow \min\:\frac{\beta}{2}\sum_{n=1}^N\{y(x_n,w)-t_n\}^2+\frac{\alpha}{2}w^Tw</script><p>故利用MAP求解参数w的过程等驾驭最小化带regularization的SSE损失（<script type="math/tex">\lambda=\alpha/\beta</script>）。</p>
<h2 id="1-2-6-Bayesian-curve-fitting"><a href="#1-2-6-Bayesian-curve-fitting" class="headerlink" title="1.2.6 Bayesian curve fitting"></a>1.2.6 Bayesian curve fitting</h2><p><strong>Assumption: </strong>参数<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>已知。</p>
<p>给定训练数据(X, T)进行模型训练，对于一个新数据点x给出其标签t的预测分布p(t|x, X, T)为<script type="math/tex">p(t|x,X,T)=\int p(t|x,w)p(w|X,T)dw</script>，其结果为</p>
<script type="math/tex; mode=display">
\begin{split}
p(t|x,X,T)=N(t|m(x),s^2(x)) \\
m(x)=\beta\phi(x)^TS\sum_{n=1}^N\phi(x_n)t_n \\
s^2(x)=\beta^{-1}+\phi(x)^TS\phi(x) \\
S^{-1}=\alpha I+\beta\sum_{n=1}^N\phi(x_n)\phi(x)^T
\end{split}</script><p>其中，vector <script type="math/tex">\phi_i(x)=x^i\:for\:i=0,...,M</script></p>
<blockquote>
<p>可以视作两个高斯分布的卷积，结果一定为高斯分布。</p>
</blockquote>
<h1 id="1-3-Model-Selection"><a href="#1-3-Model-Selection" class="headerlink" title="1.3. Model Selection"></a>1.3. Model Selection</h1><h3 id="S-fold-cross-validation（避免使用validation-set）"><a href="#S-fold-cross-validation（避免使用validation-set）" class="headerlink" title="S-fold cross-validation（避免使用validation set）"></a>S-fold cross-validation（避免使用validation set）</h3><p><strong>Def: </strong>首先将数据分为S份，每次选择S-1份用来train，剩下1份用来evaluation，如此重复S次，最终模型的performance就是这S个evaluation的平均。</p>
<p><img src="/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/cross-validation.png" alt="cross-validation"></p>
<blockquote>
<p>当S=N时也称作leave-one-out technique。</p>
</blockquote>
<p><strong>Prob: </strong></p>
<ol>
<li>the number of training runs that must be performed is increased by a factor of S</li>
<li>we might have multiple complexity params for a single model</li>
</ol>
<h1 id="1-4-The-Curse-of-Dimensionality"><a href="#1-4-The-Curse-of-Dimensionality" class="headerlink" title="1.4. The Curse of Dimensionality"></a>1.4. The Curse of Dimensionality</h1><p>Part1: 维数增长会带来运算量激增等困境</p>
<p>Part2: 低维空间中的某些intuitions在高维度空间中不适用</p>
<p>但在实际运用中，实际目标常常受限于低维空间当中，且局部光滑无突变，故并不影响我们在高维度空间中寻找有效算法。</p>
<h1 id="1-5-Decision-Theory"><a href="#1-5-Decision-Theory" class="headerlink" title="1.5. Decision Theory"></a>1.5. Decision Theory</h1><p>由Bayesian theorem可得，新数据x被划分到类别<script type="math/tex">C_k</script>的概率为</p>
<script type="math/tex; mode=display">
p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}</script><p>我们的目的是最小化分类错误的概率，那么intuitively我们应该选择后验概率更大的那类。</p>
<h2 id="1-5-1-Minimizing-the-misclassification-rate"><a href="#1-5-1-Minimizing-the-misclassification-rate" class="headerlink" title="1.5.1 Minimizing the misclassification rate"></a>1.5.1 Minimizing the misclassification rate</h2><p><strong>Aim: </strong>划分出合适的decision regions <script type="math/tex">R_k</script></p>
<ol>
<li>最小化误分类率——以二分类为例</li>
</ol>
<script type="math/tex; mode=display">
p(mistake)=p(x\in R_1,C_2)+p(x\in R_2,C_1)=\int_{R_1}p(x,C_2)dx+\int_{R_2}p(x,C_1)dx</script><ol>
<li>最大化正确分类率——以多分类为例</li>
</ol>
<script type="math/tex; mode=display">
p(correct)=\sum_{k=1}^Kp(x\in R_k,C_k)=\sum_{k=1}^{K}\int_{R_k}p(x,C_k)dx</script><p><strong>Summary: </strong>for each new x, we can choose the category <script type="math/tex">C_k</script> which can maximize <script type="math/tex">P(C_k|x)</script></p>
<h2 id="1-5-2-Minimizing-the-expected-loss"><a href="#1-5-2-Minimizing-the-expected-loss" class="headerlink" title="1.5.2 Minimizing the expected loss"></a>1.5.2 Minimizing the expected loss</h2><p><img src="/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/loss matrix.png" alt="loss matrix"></p>
<p>使用一个<strong>loss matrix</strong>来刻画误分类的cost，则loss function可以表示为</p>
<script type="math/tex; mode=display">
E[L]=\sum_k\sum_j\int_{R_j}L_{kj}p(x,C_k)dx</script><p>其中<script type="math/tex">L_{kj}</script>表示将原本属于<script type="math/tex">C_k</script>类的样本点误分类到<script type="math/tex">C_j</script>类所带来的损失。</p>
<p><strong>Summary: </strong>for each new x, we can choose the category <script type="math/tex">C_j</script> which can minimize <script type="math/tex">\sum_kL_{kj}p(C_k|x)</script></p>
<h2 id="1-5-3-The-reject-option"><a href="#1-5-3-The-reject-option" class="headerlink" title="1.5.3 The reject option"></a>1.5.3 The reject option</h2><p>面对分类问题，当最大的posterior仍然远小于1时，或者是有两个或多个posterior相近时，按照MAP原则往往会产生很多分类错误。因此可以设置一个<strong>threshold <script type="math/tex">\theta</script>（拒绝域）</strong>，拒绝对那些最大posterior仍小于或等于<script type="math/tex">\theta</script>的分类做出决策，而转交给human expert来完成。</p>
<h2 id="1-5-4-Inference-and-decision"><a href="#1-5-4-Inference-and-decision" class="headerlink" title="1.5.4 Inference and decision"></a>1.5.4 Inference and decision</h2><p><strong>将分类问题分为两步：</strong></p>
<ol>
<li><strong>Inference stage</strong>: use training data to learn a model for <script type="math/tex">P(C_k|x)</script></li>
<li><strong>Decision stage</strong>: use these posterior probabilities to make optimal calss assignments</li>
</ol>
<p><strong>三种解决分类问题的手段：</strong></p>
<p>（a）<strong>Generative model（生成式模型）：</strong>model the joint distribution <script type="math/tex">P(x,C_k)</script> and (nomalize the joint distribution) obtain the posterior: <script type="math/tex">p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}</script></p>
<p><strong>Notes: </strong>1. most demanding  2. can do the outlier detection task</p>
<p>（b）<strong>Discriminative model（判别式模型）：</strong>directly calculate the posterior <script type="math/tex">p(C_k|x)</script></p>
<p><strong>Notes: </strong>the best choice when doing the classification work</p>
<p>（c）<strong>Discriminant function（判别函数）：</strong>find f(x) which maps x onto a class label</p>
<p><strong>Notes: </strong>1. combine the inference and decision stages into a single step 2. without posterior, which is hard to apply to some complex tasks</p>
<blockquote>
<p>There are many powerful reasons for wanting to compute the posterior probabilities:</p>
<ol>
<li>MInimizing task</li>
<li>Reject option</li>
<li>Compensating for class priors</li>
<li>Combining models</li>
</ol>
</blockquote>
<h2 id="1-5-5-Loss-functions-for-regression"><a href="#1-5-5-Loss-functions-for-regression" class="headerlink" title="1.5.5 Loss functions for regression"></a>1.5.5 Loss functions for regression</h2><p>考虑回归问题，对于每个输入样本点x有模型输出y(x)和与之对应的标签t，引入期望损失并用squared loss作为损失函数：</p>
<script type="math/tex; mode=display">
E[L]=\iint\{y(x)-t\}^2p(x,t)dxdt</script><p><strong>Aim: </strong>find a y(x) which can minimize E[L]</p>
<p>将损失函数E[L]进行拆分得到：<script type="math/tex">E[L]=\int\{y(x)-E[t|x]\}^2p(x)dx+\int\{E[t|x]-t\}^2p(x)dx</script>，由此可知实际上第一项是我们的优化目标，而第二项与y(x)无关，可以视作noise。</p>
<p>故解得最优解<script type="math/tex">y(x)=E_t[t|x]</script>，y(x)也被称为regression function。</p>
<p><strong>三种解决回归问题的手段：（和分类问题类似）</strong></p>
<p>（a）Generative model</p>
<p>（b）Discriminative model</p>
<p>（c）Discriminant function</p>
<h1 id="1-6-Information-Theory"><a href="#1-6-Information-Theory" class="headerlink" title="1.6. Information Theory"></a>1.6. Information Theory</h1><h3 id="信息量h-x"><a href="#信息量h-x" class="headerlink" title="信息量h(x)"></a>信息量h(x)</h3><p>“degree of surprise”，关于事件x发生的概率呈负相关，最终给出的定义为<script type="math/tex">h(x)=-log_2p(x)</script></p>
<h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>对随机变量x的信息量求期望，得到的是entropy：<script type="math/tex">H[x]=-\sum_xp(x)log_2p(x)</script>。特别地，当p(x)=0时，H[x]=0。</p>
<blockquote>
<p>对于离散型随机变量而言，当p(x)为均匀分布时熵最大，此时H=lnM where M is the total number of states <script type="math/tex">x_i</script></p>
</blockquote>
<h3 id="微分熵"><a href="#微分熵" class="headerlink" title="微分熵"></a>微分熵</h3><p>Diffrential entropy表示具体化一个连续的随机变量需要多少bit：<script type="math/tex">H[x]=-\int p(x)lnp(x)dx</script></p>
<blockquote>
<p>对于连续型随机变量而言，当p(x)为高斯分布时熵最大，此时<script type="math/tex">H[x]=\frac{1}{2}\{1+ln(2\pi\sigma^2)\}</script></p>
</blockquote>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>Conditional entropy是指假定联合分布p(x,y)和x已知，想要确定y还需要的额外信息量为H[y|x]，定义为<script type="math/tex">H[y|x]=-\iint p(y,x)ln(y|x)dydx</script>，且满足<script type="math/tex">H[x,y]=H[y|x]+H[x]</script></p>
<h2 id="1-6-1-Relative-entropy-and-mutual-information"><a href="#1-6-1-Relative-entropy-and-mutual-information" class="headerlink" title="1.6.1 Relative entropy and mutual information"></a>1.6.1 Relative entropy and mutual information</h2><h3 id="Relative-entropy"><a href="#Relative-entropy" class="headerlink" title="Relative entropy"></a>Relative entropy</h3><p><strong>Aim: </strong>对于未知的p(x)，使用已知q(x)进行近似</p>
<p>使用p(x)和q(x)的平均信息编码长度之差来衡量二者之间的差异（用已知p(x)希望得到目标q(x)所需要的额外信息量），即<strong>relative entropy或KL divergence</strong>：</p>
<script type="math/tex; mode=display">
KL(p\lVert q)=-\int p(x)ln\Big\{\frac{q(x)}{p(x)}\Big\}dx</script><p>The KL divergence satisfies <script type="math/tex">KL(p||q)\geq0</script> with equality if, and only if, p(x)=q(x)</p>
<p>由于KL散度的计算设计繁琐的积分运算，因此考虑KL散度的近似计算方法：</p>
<script type="math/tex; mode=display">
KL(p\lVert q)\simeq \sum_{n=1}^{N}\{-lnq(x_n|\theta)+lnp(x_n)\}</script><p>其中只有第一项受到adjustable params <script type="math/tex">\theta</script>的影响，因此最小化KL散度等等价于MLE。</p>
<h3 id="Mutual-information"><a href="#Mutual-information" class="headerlink" title="Mutual information"></a>Mutual information</h3><p><strong>Aim: </strong>衡量两个不独立的变量x、y之间的“接近”程度</p>
<p><strong>Mutual information（互信息）</strong> 表示一个新观测到的y造成x不确定性减小的程度，定义为：</p>
<script type="math/tex; mode=display">
I[x,y]=-\iint p(x,y)ln\Big(\frac{p(x)p(y)}{p(x,y)}\Big)dxdy</script><script type="math/tex; mode=display">I[x,y]\geq0$$ with equality if, and only if, x and y are independent

**Relative entropy 和 mutual information 之间的联系：**$$I[x,y]=H[x]-H[x|y]=H[y]-H[y|x]</script>
    </div>

    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      </div>
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>nashswift
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/08/14/%E3%80%8APRML%E3%80%8Bch1-introduction/" title="《PRML》ch1_introduction">http://example.com/2023/08/14/《PRML》ch1-introduction/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/06/16/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%EF%BC%88AKA%EF%BC%9A%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" rel="prev" title="周志华《机器学习》（AKA：西瓜书）笔记整理-第十一章-特征选择与稀疏学习">
      <i class="fa fa-chevron-left"></i> 周志华《机器学习》（AKA：西瓜书）笔记整理-第十一章-特征选择与稀疏学习
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-1-Example-Polynomial-Curve-Fitting"><span class="nav-number">1.</span> <span class="nav-text">1.1. Example: Polynomial Curve Fitting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-number">1.0.1.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lost-function"><span class="nav-number">1.0.2.</span> <span class="nav-text">Lost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-comparison-Model-selection"><span class="nav-number">1.0.3.</span> <span class="nav-text">Model comparison &#x2F; Model selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">1.0.4.</span> <span class="nav-text">Regularization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-2-Probability-Theory"><span class="nav-number">2.</span> <span class="nav-text">1.2. Probability Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Rules-of-Probability"><span class="nav-number">2.0.1.</span> <span class="nav-text">The Rules of Probability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bayes%E2%80%99s-theorem"><span class="nav-number">2.0.2.</span> <span class="nav-text">Bayes’s theorem</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-1-Probability-densities"><span class="nav-number">2.1.</span> <span class="nav-text">1.2.1 Probability densities</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Probability-density-function-PDF"><span class="nav-number">2.1.1.</span> <span class="nav-text">Probability density function(PDF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cummulative-distribution-function-CDF"><span class="nav-number">2.1.2.</span> <span class="nav-text">Cummulative distribution function(CDF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-linear-change-of-variable%EF%BC%88-%EF%BC%89"><span class="nav-number">2.1.3.</span> <span class="nav-text">Non-linear change of variable（*）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-2-Expectations-and-covariance"><span class="nav-number">2.2.</span> <span class="nav-text">1.2.2 Expectations and covariance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Expectation"><span class="nav-number">2.2.1.</span> <span class="nav-text">Expectation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Variance"><span class="nav-number">2.2.2.</span> <span class="nav-text">Variance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-3-Bayesian-probabilities"><span class="nav-number">2.3.</span> <span class="nav-text">1.2.3 Bayesian probabilities</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-4-The-Gaussian-distribution"><span class="nav-number">2.4.</span> <span class="nav-text">1.2.4 The Gaussian distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MLE-for-Gaussian-distribution"><span class="nav-number">2.4.1.</span> <span class="nav-text">MLE for Gaussian distribution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-5-Curve-fitting-re-visited"><span class="nav-number">2.5.</span> <span class="nav-text">1.2.5 Curve fitting re-visited</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-6-Bayesian-curve-fitting"><span class="nav-number">2.6.</span> <span class="nav-text">1.2.6 Bayesian curve fitting</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-3-Model-Selection"><span class="nav-number">3.</span> <span class="nav-text">1.3. Model Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#S-fold-cross-validation%EF%BC%88%E9%81%BF%E5%85%8D%E4%BD%BF%E7%94%A8validation-set%EF%BC%89"><span class="nav-number">3.0.1.</span> <span class="nav-text">S-fold cross-validation（避免使用validation set）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-4-The-Curse-of-Dimensionality"><span class="nav-number">4.</span> <span class="nav-text">1.4. The Curse of Dimensionality</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-5-Decision-Theory"><span class="nav-number">5.</span> <span class="nav-text">1.5. Decision Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-1-Minimizing-the-misclassification-rate"><span class="nav-number">5.1.</span> <span class="nav-text">1.5.1 Minimizing the misclassification rate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-2-Minimizing-the-expected-loss"><span class="nav-number">5.2.</span> <span class="nav-text">1.5.2 Minimizing the expected loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-3-The-reject-option"><span class="nav-number">5.3.</span> <span class="nav-text">1.5.3 The reject option</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-4-Inference-and-decision"><span class="nav-number">5.4.</span> <span class="nav-text">1.5.4 Inference and decision</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-5-Loss-functions-for-regression"><span class="nav-number">5.5.</span> <span class="nav-text">1.5.5 Loss functions for regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-6-Information-Theory"><span class="nav-number">6.</span> <span class="nav-text">1.6. Information Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E9%87%8Fh-x"><span class="nav-number">6.0.1.</span> <span class="nav-text">信息量h(x)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%86%B5"><span class="nav-number">6.0.2.</span> <span class="nav-text">熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AE%E5%88%86%E7%86%B5"><span class="nav-number">6.0.3.</span> <span class="nav-text">微分熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E7%86%B5"><span class="nav-number">6.0.4.</span> <span class="nav-text">条件熵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-1-Relative-entropy-and-mutual-information"><span class="nav-number">6.1.</span> <span class="nav-text">1.6.1 Relative entropy and mutual information</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Relative-entropy"><span class="nav-number">6.1.1.</span> <span class="nav-text">Relative entropy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mutual-information"><span class="nav-number">6.1.2.</span> <span class="nav-text">Mutual information</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="nashswift"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">nashswift</p>
  <div class="site-description" itemprop="description">What matters is not your ability, but your choice.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-06 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nashswift</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共24.3k字</span>
</div>
        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
